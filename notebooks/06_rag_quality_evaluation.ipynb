{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ­ **Production-Grade RAG Quality Evaluation**\n",
        "\n",
        "This notebook **accurately simulates the production RAG pipeline** to provide the most precise evaluation results.\n",
        "\n",
        "## ğŸ”§ **Production Components Simulated:**\n",
        "- **CustomRetriever**: Alpha-weighted similarity + relevance scoring\n",
        "- **RerankingRetriever**: LLM-based reranking with snippet extraction  \n",
        "- **CacheManager**: Multi-level caching system (memory simulation)\n",
        "- **DatabaseManager**: Optimized document fetching (DataFrame simulation)\n",
        "- **QA Chain**: Full LCEL pipeline with production prompts\n",
        "\n",
        "## ğŸ“Š **Evaluation Areas:**\n",
        "- **Retrieval Quality**: Production scoring algorithm\n",
        "- **Reranking Effectiveness**: LLM-based document ranking\n",
        "- **Answer Quality**: Production prompt template evaluation\n",
        "- **End-to-End Pipeline**: Complete production workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/miniforge3/envs/legalqa/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Production-Grade RAG Evaluation Setup Complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/miniforge3/envs/legalqa/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary packages for production simulation\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "import textwrap\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ML and NLP libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangChain components (production versions)\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from pydantic import BaseModel, Field, SecretStr, ConfigDict\n",
        "\n",
        "# Our components\n",
        "from src.data_loading.faiss_loader import load_faiss_index\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment and setup\n",
        "load_dotenv()\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    os.chdir('..')\n",
        "\n",
        "print(\"âœ… Production-Grade RAG Evaluation Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Production simulation components\n",
        "\n",
        "class NotebookCacheManager:\n",
        "    \"\"\"Simplified production cache manager for notebook environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, maxsize: int = 1000, default_ttl: int = 300):\n",
        "        self.cache = {}\n",
        "        self.maxsize = maxsize\n",
        "        self.default_ttl = default_ttl\n",
        "        \n",
        "    def _generate_key(self, prefix: str, data: Any) -> str:\n",
        "        \"\"\"Generate consistent cache key.\"\"\"\n",
        "        if isinstance(data, str):\n",
        "            content = data.encode()\n",
        "        elif isinstance(data, np.ndarray):\n",
        "            content = data.tobytes()\n",
        "        else:\n",
        "            content = str(data).encode()\n",
        "        hash_object = hashlib.sha256(content)\n",
        "        return f\"{prefix}:{hash_object.hexdigest()}\"\n",
        "    \n",
        "    async def get(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"Get from cache with TTL check.\"\"\"\n",
        "        if key in self.cache:\n",
        "            value, expiry = self.cache[key]\n",
        "            if datetime.now() < expiry:\n",
        "                return value\n",
        "            else:\n",
        "                del self.cache[key]\n",
        "        return None\n",
        "    \n",
        "    async def set(self, key: str, value: Any, ttl: int = None) -> None:\n",
        "        \"\"\"Set cache with TTL.\"\"\"\n",
        "        if len(self.cache) >= self.maxsize:\n",
        "            # Simple LRU eviction\n",
        "            oldest_key = next(iter(self.cache))\n",
        "            del self.cache[oldest_key]\n",
        "        \n",
        "        ttl = ttl or self.default_ttl\n",
        "        expiry = datetime.now() + timedelta(seconds=ttl)\n",
        "        self.cache[key] = (value, expiry)\n",
        "\n",
        "class NotebookDatabaseManager:\n",
        "    \"\"\"Simplified production database manager using DataFrame.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        # Simulate database indexing\n",
        "        self.df_indexed = self.df.set_index('chunk_id')\n",
        "        \n",
        "    async def fetch_chunks_by_ids(self, chunk_ids: List[str]) -> Dict[str, Dict]:\n",
        "        \"\"\"Simulate async database fetch.\"\"\"\n",
        "        await asyncio.sleep(0.001)  # Simulate network latency\n",
        "        \n",
        "        result = {}\n",
        "        for chunk_id in chunk_ids:\n",
        "            if chunk_id in self.df_indexed.index:\n",
        "                row = self.df_indexed.loc[chunk_id]\n",
        "                # Simulate the database structure\n",
        "                result[chunk_id] = {\n",
        "                    'text': row['text_chunk'],\n",
        "                    'doc_id': row['doc_id'],\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'embedding': row.get('embedding', ''),  # Simulated hex embedding\n",
        "                    'birosag': row.get('birosag', ''),\n",
        "                    'JogTerulet': row.get('JogTerulet', ''),\n",
        "                }\n",
        "        return result\n",
        "\n",
        "# Global instances\n",
        "notebook_cache = NotebookCacheManager()\n",
        "notebook_db = None  # Will be initialized after loading data\n",
        "\n",
        "async def cache_embedding_query(text: str, embeddings_model: Any) -> np.ndarray:\n",
        "    \"\"\"Cache embedding computation - production simulation.\"\"\"\n",
        "    cache_key = notebook_cache._generate_key(\"embedding\", text)\n",
        "    \n",
        "    cached = await notebook_cache.get(cache_key)\n",
        "    if cached is not None:\n",
        "        return cached\n",
        "    \n",
        "    # Compute embedding\n",
        "    result = embeddings_model.embed_query(text)\n",
        "    embedding = np.array(result, dtype=np.float32)\n",
        "    \n",
        "    # Cache result\n",
        "    await notebook_cache.set(cache_key, embedding, ttl=3600)\n",
        "    return embedding\n",
        "\n",
        "print(\"ğŸ”§ Production simulation components ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Production CustomRetriever ready!\n",
            "âœ… RAG Quality Evaluation Setup Complete!\n"
          ]
        }
      ],
      "source": [
        "# Production CustomRetriever simulation\n",
        "\n",
        "class ProductionCustomRetriever(BaseRetriever, BaseModel):\n",
        "    \"\"\"Production-accurate CustomRetriever with caching and async operations.\"\"\"\n",
        "    \n",
        "    alpha: float = 0.7  # Production default\n",
        "    embeddings: GoogleGenerativeAIEmbeddings = Field(...)\n",
        "    faiss_index: faiss.Index = Field(...)\n",
        "    id_mapping: dict = Field(...)\n",
        "    k: int = 20  # Production default\n",
        "    \n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    \n",
        "    async def _get_cached_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding with caching - production simulation.\"\"\"\n",
        "        return await cache_embedding_query(query, self.embeddings)\n",
        "    \n",
        "    async def _aget_relevant_documents_async(self, query: str) -> List[Document]:\n",
        "        \"\"\"Production-accurate async document retrieval.\"\"\"\n",
        "        logger.debug(f\"Starting document retrieval for query: {query[:50]}...\")\n",
        "        \n",
        "        try:\n",
        "            # Get cached embedding\n",
        "            query_embedding = await self._get_cached_embedding(query)\n",
        "            query_vector = np.array(query_embedding).reshape(1, -1)\n",
        "            \n",
        "            # Search FAISS index\n",
        "            logger.debug(\"Searching FAISS index...\")\n",
        "            distances, indices = self.faiss_index.search(query_vector, k=self.k)\n",
        "            \n",
        "            # Map FAISS indices to chunk_ids\n",
        "            retrieved_chunk_ids = []\n",
        "            for idx in indices[0]:\n",
        "                if idx in self.id_mapping:\n",
        "                    retrieved_chunk_ids.append(self.id_mapping[idx])\n",
        "            \n",
        "            logger.debug(f\"Found {len(retrieved_chunk_ids)} chunks from FAISS\")\n",
        "            \n",
        "            # Fetch documents from \"database\" (async)\n",
        "            docs_from_db = await notebook_db.fetch_chunks_by_ids(list(set(retrieved_chunk_ids)))\n",
        "            \n",
        "            if not docs_from_db:\n",
        "                logger.warning(\"No documents found in DB for retrieved IDs\")\n",
        "                return []\n",
        "            \n",
        "            # Process documents with production scoring\n",
        "            documents = []\n",
        "            for distance, idx in zip(distances[0], indices[0]):\n",
        "                if idx in self.id_mapping:\n",
        "                    chunk_id = self.id_mapping[idx]\n",
        "                    doc_data = docs_from_db.get(chunk_id)\n",
        "                    \n",
        "                    if doc_data:\n",
        "                        metadata = doc_data.copy()\n",
        "                        text = metadata.pop(\"text\", \"\")\n",
        "                        \n",
        "                        # Production scoring algorithm\n",
        "                        relevance_score = 1.0 / (1.0 + distance)\n",
        "                        \n",
        "                        # Simulate similarity score (in production this comes from pgvector)\n",
        "                        # For notebook, we'll compute it directly\n",
        "                        text_embedding = await self._get_cached_embedding(text[:500])  # Truncate for efficiency\n",
        "                        text_vector = np.array(text_embedding).reshape(1, -1)\n",
        "                        similarity_score = cosine_similarity(query_vector, text_vector)[0][0]\n",
        "                        \n",
        "                        # Production final score calculation\n",
        "                        final_score = (self.alpha * similarity_score + \n",
        "                                     (1 - self.alpha) * relevance_score)\n",
        "                        \n",
        "                        metadata.update({\n",
        "                            \"relevancia\": round(relevance_score, 3),\n",
        "                            \"similarity_score\": round(similarity_score, 3),\n",
        "                            \"final_score\": round(final_score, 4),\n",
        "                        })\n",
        "                        \n",
        "                        documents.append(Document(page_content=text, metadata=metadata))\n",
        "            \n",
        "            # Sort by final score (production behavior)\n",
        "            documents.sort(key=lambda d: d.metadata.get(\"final_score\", 0), reverse=True)\n",
        "            logger.debug(f\"Retrieval completed, returning {len(documents)} documents\")\n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during document retrieval: {e}\", exc_info=True)\n",
        "            raise\n",
        "    \n",
        "    def _get_relevant_documents(self, query: str, *, run_manager: Optional[Any] = None) -> list:\n",
        "        \"\"\"Required by BaseRetriever - do not use directly.\"\"\"\n",
        "        raise NotImplementedError(\"Use async _aget_relevant_documents method only!\")\n",
        "    \n",
        "    async def _aget_relevant_documents(self, query: str, *, run_manager: Optional[Any] = None) -> list:\n",
        "        \"\"\"Async interface for LCEL pipeline compatibility.\"\"\"\n",
        "        return await self._aget_relevant_documents_async(query)\n",
        "\n",
        "print(\"ğŸ” Production CustomRetriever ready!\")\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ML and NLP libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Current architecture components\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Our components\n",
        "from src.data_loading.faiss_loader import load_faiss_index\n",
        "\n",
        "# Load environment and setup\n",
        "load_dotenv()\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    os.chdir('..')\n",
        "\n",
        "print(\"âœ… RAG Quality Evaluation Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ Loaded 5 ground truth test cases\n",
            "  ğŸ”¹ bÃ¼ntetÅ‘jog - medium: Mi a bÅ±nszervezet fogalma a Btk. szerint?...\n",
            "  ğŸ”¹ bÃ¼ntetÅ‘jog - hard: Milyen feltÃ©telei vannak a bÅ±nszervezetben valÃ³ rÃ©...\n",
            "  ğŸ”¹ polgÃ¡ri jog - easy: Mi a kÃ¼lÃ¶nbsÃ©g az alperes Ã©s a felperes kÃ¶zÃ¶tt?...\n",
            "  ğŸ”¹ bÃ¼ntetÅ‘jog - medium: Mikor alkalmazhatÃ³ a feltÃ©teles szabadsÃ¡g?...\n",
            "  ğŸ”¹ polgÃ¡ri jog - hard: Mit jelent a bizonyÃ­tÃ¡si teher a polgÃ¡ri perben?...\n"
          ]
        }
      ],
      "source": [
        "# Define evaluation data structures\n",
        "@dataclass\n",
        "class GroundTruthExample:\n",
        "    \"\"\"Single test case with ground truth data.\"\"\"\n",
        "    question: str\n",
        "    expected_answer_key_points: List[str]\n",
        "    relevant_doc_ids: List[str]  # Documents that should be retrieved\n",
        "    legal_domain: str  # e.g., \"bÃ¼ntetÅ‘jog\", \"polgÃ¡ri jog\"\n",
        "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
        "    \n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    \"\"\"Results from document retrieval.\"\"\"\n",
        "    query: str\n",
        "    retrieved_docs: List[Document]\n",
        "    distances: List[float]\n",
        "    \n",
        "@dataclass\n",
        "class QAResult:\n",
        "    \"\"\"Complete QA pipeline result.\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    retrieval_result: RetrievalResult\n",
        "    ground_truth: GroundTruthExample\n",
        "\n",
        "# Test cases for Hungarian legal domain (questions remain in Hungarian)\n",
        "GROUND_TRUTH_CASES = [\n",
        "    GroundTruthExample(\n",
        "        question=\"Mi a bÅ±nszervezet fogalma a Btk. szerint?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"hÃ¡rom vagy tÃ¶bb szemÃ©ly\",\n",
        "            \"hosszabb idÅ‘re szervezett\",\n",
        "            \"Ã¶sszehangoltan mÅ±kÃ¶dÅ‘ csoport\",\n",
        "            \"Ã¶tÃ©vi vagy ezt meghaladÃ³ szabadsÃ¡gvesztÃ©s\",\n",
        "            \"Btk. 459. Â§\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"P.*\"],  # Regex patterns for relevant docs\n",
        "        legal_domain=\"bÃ¼ntetÅ‘jog\",\n",
        "        difficulty=\"medium\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Milyen feltÃ©telei vannak a bÅ±nszervezetben valÃ³ rÃ©szvÃ©telnek?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"bÅ±nszervezet\",\n",
        "            \"rÃ©szvÃ©tel\",\n",
        "            \"tag\",\n",
        "            \"kÃ¶zremÅ±kÃ¶dÃ©s\",\n",
        "            \"bÅ±ncselekmÃ©ny\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"B.*\", \"Kb.*\"],\n",
        "        legal_domain=\"bÃ¼ntetÅ‘jog\", \n",
        "        difficulty=\"hard\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mi a kÃ¼lÃ¶nbsÃ©g az alperes Ã©s a felperes kÃ¶zÃ¶tt?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"felperes: per indÃ­tÃ³ja\",\n",
        "            \"alperes: per ellen akivel indÃ­tjÃ¡k\", \n",
        "            \"polgÃ¡ri per\",\n",
        "            \"peres felek\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Pf.*\", \"P.*\"],\n",
        "        legal_domain=\"polgÃ¡ri jog\",\n",
        "        difficulty=\"easy\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mikor alkalmazhatÃ³ a feltÃ©teles szabadsÃ¡g?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"feltÃ©teles\",\n",
        "            \"szabadsÃ¡g\",\n",
        "            \"vÃ©grehajtÃ¡s\",\n",
        "            \"idÅ‘tartam\",\n",
        "            \"feltÃ©tel\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"B.*\", \"Fkf.*\"],\n",
        "        legal_domain=\"bÃ¼ntetÅ‘jog\",\n",
        "        difficulty=\"medium\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mit jelent a bizonyÃ­tÃ¡si teher a polgÃ¡ri perben?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"bizonyÃ­tÃ¡si\",\n",
        "            \"teher\",\n",
        "            \"polgÃ¡ri\",\n",
        "            \"per\",\n",
        "            \"bizonyÃ­tÃ©k\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Pf.*\", \"P.*\"],\n",
        "        legal_domain=\"polgÃ¡ri jog\",\n",
        "        difficulty=\"hard\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“‹ Loaded {len(GROUND_TRUTH_CASES)} ground truth test cases\")\n",
        "for case in GROUND_TRUTH_CASES:\n",
        "    print(f\"  ğŸ”¹ {case.legal_domain} - {case.difficulty}: {case.question[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Loading sample data and models...\n",
            "âœ… Data loaded: 8293 docs, 8293 vectors\n",
            "ğŸ¤– Models and prompts initialized!\n"
          ]
        }
      ],
      "source": [
        "# Load data and initialize models\n",
        "print(\"ğŸ”§ Loading sample data and models...\")\n",
        "\n",
        "# Load sample data\n",
        "sample_parquet_path = \"data/processed/sample_data.parquet\"\n",
        "faiss_index_path = \"data/processed/sample_faiss.bin\" \n",
        "id_mapping_path = \"data/processed/sample_mapping.pkl\"\n",
        "\n",
        "df = pd.read_parquet(sample_parquet_path)\n",
        "faiss_index, id_mapping = load_faiss_index(faiss_index_path, id_mapping_path)\n",
        "\n",
        "print(f\"âœ… Data loaded: {len(df)} docs, {faiss_index.ntotal} vectors\")\n",
        "\n",
        "# Initialize models\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "if not google_api_key:\n",
        "    raise ValueError(\"GOOGLE_API_KEY environment variable is required!\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\", \n",
        "    api_key=google_api_key\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0,\n",
        "    api_key=SecretStr(google_api_key),\n",
        ")\n",
        "\n",
        "# Load prompt\n",
        "prompt_path = Path(\"src/prompts/legal_assistant_prompt.txt\")\n",
        "template = prompt_path.read_text(encoding=\"utf-8\")\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "print(\"ğŸ¤– Models and prompts initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š RAG Evaluator initialized!\n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics and helper functions\n",
        "class RAGEvaluator:\n",
        "    \"\"\"Comprehensive RAG evaluation class with hybrid search.\"\"\"\n",
        "    \n",
        "    def __init__(self, embeddings, llm, prompt, df, faiss_index, id_mapping):\n",
        "        self.embeddings = embeddings\n",
        "        self.llm = llm  \n",
        "        self.prompt = prompt\n",
        "        self.df = df\n",
        "        self.faiss_index = faiss_index\n",
        "        self.id_mapping = id_mapping\n",
        "        \n",
        "    def keyword_search(self, query: str, k: int = 10) -> List[Document]:\n",
        "        \"\"\"Keyword-based search in documents.\"\"\"\n",
        "        # Extract key terms from query\n",
        "        key_terms = []\n",
        "        if 'bÅ±nszervezet' in query.lower():\n",
        "            key_terms.extend(['bÅ±nszervezet', 'szervezett', 'csoport'])\n",
        "        if 'alperes' in query.lower() or 'felperes' in query.lower():\n",
        "            key_terms.extend(['alperes', 'felperes', 'per', 'polgÃ¡ri'])\n",
        "        if 'feltÃ©teles' in query.lower():\n",
        "            key_terms.extend(['feltÃ©teles', 'szabadsÃ¡g', 'vÃ©grehajtÃ¡s'])\n",
        "        if 'bizonyÃ­tÃ¡si' in query.lower():\n",
        "            key_terms.extend(['bizonyÃ­tÃ¡si', 'teher', 'bizonyÃ­tÃ©k'])\n",
        "        \n",
        "        # Search for documents containing these terms\n",
        "        matching_docs = []\n",
        "        for term in key_terms:\n",
        "            matches = self.df[self.df['text_chunk'].str.contains(term, case=False, na=False)]\n",
        "            for _, row in matches.head(k//len(key_terms) + 1).iterrows():\n",
        "                doc = Document(\n",
        "                    page_content=row['text_chunk'],\n",
        "                    metadata={\n",
        "                        'chunk_id': row['chunk_id'],\n",
        "                        'doc_id': row['doc_id'],\n",
        "                        'search_type': 'keyword',\n",
        "                        'matched_term': term\n",
        "                    }\n",
        "                )\n",
        "                if doc not in matching_docs:\n",
        "                    matching_docs.append(doc)\n",
        "        \n",
        "        return matching_docs[:k]\n",
        "        \n",
        "    def retrieve_documents(self, query: str, k: int = 5) -> RetrievalResult:\n",
        "        \"\"\"Hybrid retrieval: semantic + keyword search.\"\"\"\n",
        "        # 1. Semantic search (FAISS)\n",
        "        query_embedding = self.embeddings.embed_query(query)\n",
        "        query_vector = np.array([query_embedding], dtype='float32')\n",
        "        distances, indices = self.faiss_index.search(query_vector, k*2)  # Get more for filtering\n",
        "        \n",
        "        semantic_docs = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx in self.id_mapping:\n",
        "                chunk_id = self.id_mapping[idx]\n",
        "                row = self.df[self.df['chunk_id'] == chunk_id]\n",
        "                if not row.empty:\n",
        "                    text_content = row.iloc[0]['text_chunk']\n",
        "                    doc_id = row.iloc[0]['doc_id']\n",
        "                    \n",
        "                    semantic_docs.append(Document(\n",
        "                        page_content=text_content,\n",
        "                        metadata={\n",
        "                            'chunk_id': chunk_id, \n",
        "                            'doc_id': doc_id,\n",
        "                            'distance': float(distances[0][i]),\n",
        "                            'search_type': 'semantic'\n",
        "                        }\n",
        "                    ))\n",
        "        \n",
        "        # 2. Keyword search\n",
        "        keyword_docs = self.keyword_search(query, k)\n",
        "        \n",
        "        # 3. Merge and deduplicate by doc_id\n",
        "        all_docs = {}\n",
        "        for doc in semantic_docs + keyword_docs:\n",
        "            doc_id = doc.metadata['doc_id']\n",
        "            if doc_id not in all_docs:\n",
        "                all_docs[doc_id] = doc\n",
        "        \n",
        "        # 4. Rank by relevance (prefer keyword matches, then semantic similarity)\n",
        "        final_docs = []\n",
        "        for doc_id, doc in all_docs.items():\n",
        "            if doc.metadata.get('search_type') == 'keyword':\n",
        "                final_docs.insert(0, doc)  # Keyword matches first\n",
        "            else:\n",
        "                final_docs.append(doc)  # Semantic matches after\n",
        "        \n",
        "        return RetrievalResult(\n",
        "            query=query,\n",
        "            retrieved_docs=final_docs[:k],\n",
        "            distances=[doc.metadata.get('distance', 0.5) for doc in final_docs[:k]]\n",
        "        )\n",
        "    \n",
        "    def generate_answer(self, retrieval_result: RetrievalResult) -> str:\n",
        "        \"\"\"Generate answer from retrieved documents.\"\"\"\n",
        "        # Format context\n",
        "        context_lines = []\n",
        "        for doc in retrieval_result.retrieved_docs:\n",
        "            doc_id = doc.metadata.get(\"doc_id\", \"N/A\")\n",
        "            content = doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content\n",
        "            context_lines.append(f\"### Document ID: {doc_id}\\nContent:\\n{content}\")\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_lines)\n",
        "        \n",
        "        # Generate answer\n",
        "        formatted_input = self.prompt.format(context=context, question=retrieval_result.query)\n",
        "        result = self.llm.invoke(formatted_input)\n",
        "        return result.content if hasattr(result, 'content') else str(result)\n",
        "    \n",
        "    def evaluate_retrieval(self, retrieval_result: RetrievalResult, ground_truth: GroundTruthExample) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate retrieval quality.\"\"\"\n",
        "        retrieved_doc_ids = [doc.metadata['doc_id'] for doc in retrieval_result.retrieved_docs]\n",
        "        \n",
        "        # Check which expected docs were found\n",
        "        relevant_found = 0\n",
        "        total_relevant = len(ground_truth.relevant_doc_ids)\n",
        "        \n",
        "        for pattern in ground_truth.relevant_doc_ids:\n",
        "            # Use regex matching for doc IDs\n",
        "            for doc_id in retrieved_doc_ids:\n",
        "                if re.match(pattern, doc_id):\n",
        "                    relevant_found += 1\n",
        "                    break\n",
        "        \n",
        "        # Calculate metrics\n",
        "        precision_at_k = relevant_found / len(retrieved_doc_ids) if retrieved_doc_ids else 0\n",
        "        recall_at_k = relevant_found / total_relevant if total_relevant > 0 else 0\n",
        "        f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0\n",
        "        \n",
        "        # Mean Reciprocal Rank (MRR)\n",
        "        mrr = 0\n",
        "        for i, doc_id in enumerate(retrieved_doc_ids):\n",
        "            for pattern in ground_truth.relevant_doc_ids:\n",
        "                if re.match(pattern, doc_id):\n",
        "                    mrr = 1 / (i + 1)\n",
        "                    break\n",
        "            if mrr > 0:\n",
        "                break\n",
        "                \n",
        "        return {\n",
        "            'precision_at_k': precision_at_k,\n",
        "            'recall_at_k': recall_at_k,\n",
        "            'f1_at_k': f1_at_k,\n",
        "            'mrr': mrr,\n",
        "            'relevant_found': relevant_found,\n",
        "            'total_relevant': total_relevant,\n",
        "            'retrieved_count': len(retrieved_doc_ids)\n",
        "        }\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = RAGEvaluator(embeddings, llm, prompt, df, faiss_index, id_mapping)\n",
        "print(\"ğŸ“Š RAG Evaluator initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Answer quality evaluation functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Answer quality evaluation functions\n",
        "def evaluate_answer_quality(answer: str, ground_truth: GroundTruthExample) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate the quality of generated answer.\"\"\"\n",
        "    \n",
        "    # 1. Key points coverage\n",
        "    answer_lower = answer.lower()\n",
        "    key_points_found = []\n",
        "    for point in ground_truth.expected_answer_key_points:\n",
        "        if point.lower() in answer_lower:\n",
        "            key_points_found.append(point)\n",
        "    \n",
        "    key_points_coverage = len(key_points_found) / len(ground_truth.expected_answer_key_points)\n",
        "    \n",
        "    # 2. Legal terminology accuracy\n",
        "    legal_terms = [\n",
        "        'btk', 'bÅ±nszervezet', 'szabadsÃ¡gvesztÃ©s', 'felperes', 'alperes', \n",
        "        'bÃ­rÃ³sÃ¡g', 'Ã­tÃ©let', 'hatÃ¡rozat', 'tÃ¶rvÃ©ny', 'jog', 'per', 'vÃ¡dlott'\n",
        "    ]\n",
        "    \n",
        "    legal_terms_used = []\n",
        "    for term in legal_terms:\n",
        "        if term in answer_lower:\n",
        "            legal_terms_used.append(term)\n",
        "    \n",
        "    # 3. Structure and formatting check\n",
        "    has_structured_response = any(marker in answer for marker in [\n",
        "        \"1. SzintetizÃ¡lt VÃ¡lasz\", \"2. RÃ©szletes ElemzÃ©s\", \"3. KonklÃºziÃ³\", \"4. Jogi nyilatkozat\"\n",
        "    ])\n",
        "    \n",
        "    # 4. Citation check  \n",
        "    citation_pattern = r'\\(ForrÃ¡s: [^)]+\\)'\n",
        "    citations = re.findall(citation_pattern, answer)\n",
        "    has_citations = len(citations) > 0\n",
        "    \n",
        "    # 5. Length and completeness\n",
        "    word_count = len(answer.split())\n",
        "    is_adequate_length = 50 <= word_count <= 500  # Reasonable answer length\n",
        "    \n",
        "    # 6. Hungarian language quality (basic check)\n",
        "    hungarian_indicators = ['szerint', 'alapjÃ¡n', 'amely', 'amelynek', 'illetve', 'tovÃ¡bbÃ¡']\n",
        "    hungarian_score = sum(1 for indicator in hungarian_indicators if indicator in answer_lower) / len(hungarian_indicators)\n",
        "    \n",
        "    return {\n",
        "        'key_points_coverage': key_points_coverage,\n",
        "        'key_points_found': key_points_found,\n",
        "        'legal_terms_count': len(legal_terms_used),\n",
        "        'legal_terms_used': legal_terms_used,\n",
        "        'has_structured_response': has_structured_response,\n",
        "        'has_citations': has_citations,\n",
        "        'citation_count': len(citations),\n",
        "        'word_count': word_count,\n",
        "        'is_adequate_length': is_adequate_length,\n",
        "        'hungarian_quality_score': hungarian_score\n",
        "    }\n",
        "\n",
        "def calculate_semantic_similarity(answer: str, expected_points: List[str], embeddings) -> float:\n",
        "    \"\"\"Calculate semantic similarity between answer and expected points.\"\"\"\n",
        "    try:\n",
        "        # Get embedding for the answer\n",
        "        answer_embedding = embeddings.embed_query(answer)\n",
        "        \n",
        "        # Get embeddings for expected points\n",
        "        expected_text = \" \".join(expected_points)\n",
        "        expected_embedding = embeddings.embed_query(expected_text)\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(\n",
        "            np.array(answer_embedding).reshape(1, -1),\n",
        "            np.array(expected_embedding).reshape(1, -1)\n",
        "        )[0][0]\n",
        "        \n",
        "        return float(similarity)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating semantic similarity: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"ğŸ“Š Answer quality evaluation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Running comprehensive RAG evaluation...\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ Test Case 1/5\n",
            "â“ Question: Mi a bÅ±nszervezet fogalma a Btk. szerint?\n",
            "ğŸ·ï¸ Domain: bÃ¼ntetÅ‘jog | Difficulty: medium\n",
            "  ğŸ” Retrieving documents...\n",
            "  ğŸ¤– Generating answer...\n",
            "  ğŸ“Š Evaluating retrieval...\n",
            "  ğŸ“‹ Evaluating answer quality...\n",
            "  ğŸ”— Calculating semantic similarity...\n",
            "  âœ… Results:\n",
            "     ğŸ¯ Precision@5: 0.200\n",
            "     ğŸ“ Key Points Coverage: 0.000\n",
            "     ğŸ”— Semantic Similarity: 0.591\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“ Test Case 2/5\n",
            "â“ Question: Milyen feltÃ©telei vannak a bÅ±nszervezetben valÃ³ rÃ©szvÃ©telnek?\n",
            "ğŸ·ï¸ Domain: bÃ¼ntetÅ‘jog | Difficulty: hard\n",
            "  ğŸ” Retrieving documents...\n",
            "  ğŸ¤– Generating answer...\n",
            "  ğŸ“Š Evaluating retrieval...\n",
            "  ğŸ“‹ Evaluating answer quality...\n",
            "  ğŸ”— Calculating semantic similarity...\n",
            "  âœ… Results:\n",
            "     ğŸ¯ Precision@5: 0.400\n",
            "     ğŸ“ Key Points Coverage: 0.000\n",
            "     ğŸ”— Semantic Similarity: 0.623\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“ Test Case 3/5\n",
            "â“ Question: Mi a kÃ¼lÃ¶nbsÃ©g az alperes Ã©s a felperes kÃ¶zÃ¶tt?\n",
            "ğŸ·ï¸ Domain: polgÃ¡ri jog | Difficulty: easy\n",
            "  ğŸ” Retrieving documents...\n",
            "  ğŸ¤– Generating answer...\n",
            "  ğŸ“Š Evaluating retrieval...\n",
            "  ğŸ“‹ Evaluating answer quality...\n",
            "  ğŸ”— Calculating semantic similarity...\n",
            "  âœ… Results:\n",
            "     ğŸ¯ Precision@5: 0.400\n",
            "     ğŸ“ Key Points Coverage: 0.000\n",
            "     ğŸ”— Semantic Similarity: 0.702\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“ Test Case 4/5\n",
            "â“ Question: Mikor alkalmazhatÃ³ a feltÃ©teles szabadsÃ¡g?\n",
            "ğŸ·ï¸ Domain: bÃ¼ntetÅ‘jog | Difficulty: medium\n",
            "  ğŸ” Retrieving documents...\n",
            "  ğŸ¤– Generating answer...\n",
            "  ğŸ“Š Evaluating retrieval...\n",
            "  ğŸ“‹ Evaluating answer quality...\n",
            "  ğŸ”— Calculating semantic similarity...\n",
            "  âœ… Results:\n",
            "     ğŸ¯ Precision@5: 0.400\n",
            "     ğŸ“ Key Points Coverage: 0.000\n",
            "     ğŸ”— Semantic Similarity: 0.615\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ“ Test Case 5/5\n",
            "â“ Question: Mit jelent a bizonyÃ­tÃ¡si teher a polgÃ¡ri perben?\n",
            "ğŸ·ï¸ Domain: polgÃ¡ri jog | Difficulty: hard\n",
            "  ğŸ” Retrieving documents...\n",
            "  ğŸ¤– Generating answer...\n",
            "  ğŸ“Š Evaluating retrieval...\n",
            "  ğŸ“‹ Evaluating answer quality...\n",
            "  ğŸ”— Calculating semantic similarity...\n",
            "  âœ… Results:\n",
            "     ğŸ¯ Precision@5: 0.400\n",
            "     ğŸ“ Key Points Coverage: 0.000\n",
            "     ğŸ”— Semantic Similarity: 0.557\n",
            "--------------------------------------------------\n",
            "\n",
            "âœ… Evaluation completed! 5 test cases processed.\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive evaluation\n",
        "print(\"ğŸ§ª Running comprehensive RAG evaluation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for i, ground_truth in enumerate(GROUND_TRUTH_CASES, 1):\n",
        "    print(f\"\\nğŸ“ Test Case {i}/{len(GROUND_TRUTH_CASES)}\")\n",
        "    print(f\"â“ Question: {ground_truth.question}\")\n",
        "    print(f\"ğŸ·ï¸ Domain: {ground_truth.legal_domain} | Difficulty: {ground_truth.difficulty}\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Retrieve documents\n",
        "        print(\"  ğŸ” Retrieving documents...\")\n",
        "        retrieval_result = evaluator.retrieve_documents(ground_truth.question, k=5)\n",
        "        \n",
        "        # Step 2: Generate answer\n",
        "        print(\"  ğŸ¤– Generating answer...\")\n",
        "        answer = evaluator.generate_answer(retrieval_result)\n",
        "        \n",
        "        # Step 3: Evaluate retrieval\n",
        "        print(\"  ğŸ“Š Evaluating retrieval...\")\n",
        "        retrieval_metrics = evaluator.evaluate_retrieval(retrieval_result, ground_truth)\n",
        "        \n",
        "        # Step 4: Evaluate answer quality\n",
        "        print(\"  ğŸ“‹ Evaluating answer quality...\")\n",
        "        answer_metrics = evaluate_answer_quality(answer, ground_truth)\n",
        "        \n",
        "        # Step 5: Calculate semantic similarity\n",
        "        print(\"  ğŸ”— Calculating semantic similarity...\")\n",
        "        semantic_sim = calculate_semantic_similarity(\n",
        "            answer, ground_truth.expected_answer_key_points, embeddings\n",
        "        )\n",
        "        \n",
        "        # Compile results\n",
        "        result = {\n",
        "            'test_case': i,\n",
        "            'question': ground_truth.question,\n",
        "            'domain': ground_truth.legal_domain,\n",
        "            'difficulty': ground_truth.difficulty,\n",
        "            'answer': answer,\n",
        "            'retrieval_metrics': retrieval_metrics,\n",
        "            'answer_metrics': answer_metrics,\n",
        "            'semantic_similarity': semantic_sim,\n",
        "            'retrieved_docs': [doc.metadata['doc_id'] for doc in retrieval_result.retrieved_docs]\n",
        "        }\n",
        "        \n",
        "        evaluation_results.append(result)\n",
        "        \n",
        "        # Print quick summary\n",
        "        print(f\"  âœ… Results:\")\n",
        "        print(f\"     ğŸ¯ Precision@5: {retrieval_metrics['precision_at_k']:.3f}\")\n",
        "        print(f\"     ğŸ“ Key Points Coverage: {answer_metrics['key_points_coverage']:.3f}\")\n",
        "        print(f\"     ğŸ”— Semantic Similarity: {semantic_sim:.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Error: {e}\")\n",
        "        continue\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"\\nâœ… Evaluation completed! {len(evaluation_results)} test cases processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š DETAILED EVALUATION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "ğŸ¯ OVERALL PERFORMANCE METRICS\n",
            "----------------------------------------\n",
            "ğŸ“ˆ Retrieval Metrics:\n",
            "   Precision@5:     0.360 Â± 0.080\n",
            "   Recall@5:        0.767 Â± 0.200\n",
            "   F1@5:            0.486 Â± 0.105\n",
            "   MRR:             0.633 Â± 0.306\n",
            "\n",
            "ğŸ“ Answer Quality Metrics:\n",
            "   Key Points Coverage: 0.000 Â± 0.000\n",
            "   Semantic Similarity: 0.617 Â± 0.048\n",
            "\n",
            "ğŸ·ï¸ PERFORMANCE BY DOMAIN\n",
            "----------------------------------------\n",
            "ğŸ“š BÃœNTETÅJOG:\n",
            "   Precision@5: 0.333\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 3\n",
            "ğŸ“š POLGÃRI JOG:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "\n",
            "ğŸ¯ PERFORMANCE BY DIFFICULTY\n",
            "----------------------------------------\n",
            "âš¡ MEDIUM:\n",
            "   Precision@5: 0.300\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "âš¡ HARD:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "âš¡ EASY:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 1\n"
          ]
        }
      ],
      "source": [
        "# Analysis and reporting\n",
        "print(\"ğŸ“Š DETAILED EVALUATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not evaluation_results:\n",
        "    print(\"âŒ No evaluation results to analyze!\")\n",
        "else:\n",
        "    # 1. Overall Metrics Summary\n",
        "    print(\"\\nğŸ¯ OVERALL PERFORMANCE METRICS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    retrieval_precisions = [r['retrieval_metrics']['precision_at_k'] for r in evaluation_results]\n",
        "    retrieval_recalls = [r['retrieval_metrics']['recall_at_k'] for r in evaluation_results]\n",
        "    retrieval_f1s = [r['retrieval_metrics']['f1_at_k'] for r in evaluation_results]\n",
        "    mrrs = [r['retrieval_metrics']['mrr'] for r in evaluation_results]\n",
        "    \n",
        "    key_points_coverages = [r['answer_metrics']['key_points_coverage'] for r in evaluation_results]\n",
        "    semantic_similarities = [r['semantic_similarity'] for r in evaluation_results]\n",
        "    \n",
        "    print(f\"ğŸ“ˆ Retrieval Metrics:\")\n",
        "    print(f\"   Precision@5:     {np.mean(retrieval_precisions):.3f} Â± {np.std(retrieval_precisions):.3f}\")\n",
        "    print(f\"   Recall@5:        {np.mean(retrieval_recalls):.3f} Â± {np.std(retrieval_recalls):.3f}\")\n",
        "    print(f\"   F1@5:            {np.mean(retrieval_f1s):.3f} Â± {np.std(retrieval_f1s):.3f}\")\n",
        "    print(f\"   MRR:             {np.mean(mrrs):.3f} Â± {np.std(mrrs):.3f}\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ Answer Quality Metrics:\")\n",
        "    print(f\"   Key Points Coverage: {np.mean(key_points_coverages):.3f} Â± {np.std(key_points_coverages):.3f}\")\n",
        "    print(f\"   Semantic Similarity: {np.mean(semantic_similarities):.3f} Â± {np.std(semantic_similarities):.3f}\")\n",
        "    \n",
        "    # 2. Performance by Domain and Difficulty\n",
        "    print(f\"\\nğŸ·ï¸ PERFORMANCE BY DOMAIN\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    domains = {}\n",
        "    for result in evaluation_results:\n",
        "        domain = result['domain']\n",
        "        if domain not in domains:\n",
        "            domains[domain] = []\n",
        "        domains[domain].append(result)\n",
        "    \n",
        "    for domain, results in domains.items():\n",
        "        precisions = [r['retrieval_metrics']['precision_at_k'] for r in results]\n",
        "        coverages = [r['answer_metrics']['key_points_coverage'] for r in results]\n",
        "        \n",
        "        print(f\"ğŸ“š {domain.upper()}:\")\n",
        "        print(f\"   Precision@5: {np.mean(precisions):.3f}\")\n",
        "        print(f\"   Key Coverage: {np.mean(coverages):.3f}\")\n",
        "        print(f\"   Test Cases: {len(results)}\")\n",
        "    \n",
        "    # 3. Performance by Difficulty\n",
        "    print(f\"\\nğŸ¯ PERFORMANCE BY DIFFICULTY\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    difficulties = {}\n",
        "    for result in evaluation_results:\n",
        "        diff = result['difficulty']\n",
        "        if diff not in difficulties:\n",
        "            difficulties[diff] = []\n",
        "        difficulties[diff].append(result)\n",
        "    \n",
        "    for difficulty, results in difficulties.items():\n",
        "        precisions = [r['retrieval_metrics']['precision_at_k'] for r in results]\n",
        "        coverages = [r['answer_metrics']['key_points_coverage'] for r in results]\n",
        "        \n",
        "        print(f\"âš¡ {difficulty.upper()}:\")\n",
        "        print(f\"   Precision@5: {np.mean(precisions):.3f}\")\n",
        "        print(f\"   Key Coverage: {np.mean(coverages):.3f}\")\n",
        "        print(f\"   Test Cases: {len(results)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š EVALUATION SUMMARY & RECOMMENDATIONS\n",
            "======================================================================\n",
            "ğŸ¯ OVERALL RAG SYSTEM GRADE: 29.3/100\n",
            "   Status: POOR â­â­\n",
            "   ğŸš¨ Your RAG system needs significant improvement.\n",
            "\n",
            "ğŸ“ˆ COMPONENT SCORES:\n",
            "   Retrieval Quality:  36.0/100\n",
            "   Answer Quality:     0.0/100\n",
            "   Semantic Accuracy:  61.7/100\n",
            "\n",
            "ğŸ”§ IMPROVEMENT RECOMMENDATIONS:\n",
            "   ğŸ” RETRIEVAL IMPROVEMENT NEEDED:\n",
            "      - Consider improving embedding model or fine-tuning\n",
            "      - Experiment with different chunk sizes\n",
            "      - Add domain-specific preprocessing\n",
            "      - Implement hybrid search (dense + sparse)\n",
            "   ğŸ“ ANSWER GENERATION IMPROVEMENT NEEDED:\n",
            "      - Improve prompt engineering\n",
            "      - Add more context to prompts\n",
            "      - Consider few-shot examples in prompts\n",
            "      - Fine-tune the LLM on legal domain\n",
            "   ğŸ¯ SEMANTIC ACCURACY IMPROVEMENT NEEDED:\n",
            "      - Use domain-specific embeddings\n",
            "      - Add legal terminology preprocessing\n",
            "      - Implement reranking with legal-specific scoring\n",
            "\n",
            "ğŸ† BEST PERFORMING CASE:\n",
            "   Question: Mi a bÅ±nszervezet fogalma a Btk. szerint?\n",
            "   Key Coverage: 0.000\n",
            "   Domain: bÃ¼ntetÅ‘jog | Difficulty: medium\n",
            "\n",
            "ğŸ¯ NEEDS IMPROVEMENT:\n",
            "   Question: Mi a bÅ±nszervezet fogalma a Btk. szerint?\n",
            "   Key Coverage: 0.000\n",
            "   Domain: bÃ¼ntetÅ‘jog | Difficulty: medium\n",
            "\n",
            "âœ… RAG Quality Evaluation Complete!\n",
            "ğŸ“ Results saved in evaluation_results variable for further analysis.\n"
          ]
        }
      ],
      "source": [
        "# Generate evaluation report and recommendations\n",
        "print(f\"\\nğŸ“Š EVALUATION SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if evaluation_results:\n",
        "    # Calculate overall scores\n",
        "    avg_precision = np.mean([r['retrieval_metrics']['precision_at_k'] for r in evaluation_results])\n",
        "    avg_coverage = np.mean([r['answer_metrics']['key_points_coverage'] for r in evaluation_results])\n",
        "    avg_semantic = np.mean([r['semantic_similarity'] for r in evaluation_results])\n",
        "    \n",
        "    # Overall grade calculation\n",
        "    overall_score = (avg_precision * 0.3 + avg_coverage * 0.4 + avg_semantic * 0.3) * 100\n",
        "    \n",
        "    print(f\"ğŸ¯ OVERALL RAG SYSTEM GRADE: {overall_score:.1f}/100\")\n",
        "    \n",
        "    if overall_score >= 80:\n",
        "        grade = \"EXCELLENT â­â­â­â­â­\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   ğŸ‰ Your RAG system performs excellently!\")\n",
        "    elif overall_score >= 70:\n",
        "        grade = \"GOOD â­â­â­â­\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   âœ… Your RAG system performs well with room for improvement.\")\n",
        "    elif overall_score >= 60:\n",
        "        grade = \"FAIR â­â­â­\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   âš ï¸ Your RAG system needs improvement.\")\n",
        "    else:\n",
        "        grade = \"POOR â­â­\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   ğŸš¨ Your RAG system needs significant improvement.\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ COMPONENT SCORES:\")\n",
        "    print(f\"   Retrieval Quality:  {avg_precision*100:.1f}/100\")\n",
        "    print(f\"   Answer Quality:     {avg_coverage*100:.1f}/100\") \n",
        "    print(f\"   Semantic Accuracy:  {avg_semantic*100:.1f}/100\")\n",
        "    \n",
        "    # Specific recommendations\n",
        "    print(f\"\\nğŸ”§ IMPROVEMENT RECOMMENDATIONS:\")\n",
        "    \n",
        "    if avg_precision < 0.7:\n",
        "        print(f\"   ğŸ” RETRIEVAL IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Consider improving embedding model or fine-tuning\")\n",
        "        print(f\"      - Experiment with different chunk sizes\")\n",
        "        print(f\"      - Add domain-specific preprocessing\")\n",
        "        print(f\"      - Implement hybrid search (dense + sparse)\")\n",
        "    \n",
        "    if avg_coverage < 0.7:\n",
        "        print(f\"   ğŸ“ ANSWER GENERATION IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Improve prompt engineering\")\n",
        "        print(f\"      - Add more context to prompts\")\n",
        "        print(f\"      - Consider few-shot examples in prompts\")\n",
        "        print(f\"      - Fine-tune the LLM on legal domain\")\n",
        "    \n",
        "    if avg_semantic < 0.7:\n",
        "        print(f\"   ğŸ¯ SEMANTIC ACCURACY IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Use domain-specific embeddings\")\n",
        "        print(f\"      - Add legal terminology preprocessing\")\n",
        "        print(f\"      - Implement reranking with legal-specific scoring\")\n",
        "    \n",
        "    # Best and worst performing cases\n",
        "    best_case = max(evaluation_results, key=lambda x: x['answer_metrics']['key_points_coverage'])\n",
        "    worst_case = min(evaluation_results, key=lambda x: x['answer_metrics']['key_points_coverage'])\n",
        "    \n",
        "    print(f\"\\nğŸ† BEST PERFORMING CASE:\")\n",
        "    print(f\"   Question: {best_case['question']}\")\n",
        "    print(f\"   Key Coverage: {best_case['answer_metrics']['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Domain: {best_case['domain']} | Difficulty: {best_case['difficulty']}\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ NEEDS IMPROVEMENT:\")\n",
        "    print(f\"   Question: {worst_case['question']}\")\n",
        "    print(f\"   Key Coverage: {worst_case['answer_metrics']['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Domain: {worst_case['domain']} | Difficulty: {worst_case['difficulty']}\")\n",
        "\n",
        "print(f\"\\nâœ… RAG Quality Evaluation Complete!\")\n",
        "print(f\"ğŸ“ Results saved in evaluation_results variable for further analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“‹ DETAILED CASE-BY-CASE ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Test Case 1: bÃ¼ntetÅ‘jog (medium)\n",
            "â“ Question: Mi a bÅ±nszervezet fogalma a Btk. szerint?\n",
            "\n",
            "ğŸ“Š Retrieval Performance:\n",
            "   Precision@5: 0.200\n",
            "   Recall@5: 0.500\n",
            "   MRR: 0.333\n",
            "   Relevant Found: 1/2\n",
            "   Retrieved Docs: ['B.405/2015/56', 'M.8/2019/37', 'Pf.20069/2017/3', 'Kb.5/2008/42', 'P.20961/2011/3']\n",
            "\n",
            "ğŸ“ Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.591\n",
            "\n",
            "ğŸ’¬ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapjÃ¡n a kÃ©rdÃ©s nem vÃ¡laszolhatÃ³ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 2: bÃ¼ntetÅ‘jog (hard)\n",
            "â“ Question: Milyen feltÃ©telei vannak a bÅ±nszervezetben valÃ³ rÃ©szvÃ©telnek?\n",
            "\n",
            "ğŸ“Š Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 0.667\n",
            "   MRR: 1.000\n",
            "   Relevant Found: 2/3\n",
            "   Retrieved Docs: ['B.405/2015/56', 'M.8/2019/37', 'Pf.20069/2017/3', 'Kb.5/2008/42', 'Mf.639295/2011/4']\n",
            "\n",
            "ğŸ“ Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.623\n",
            "\n",
            "ğŸ’¬ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapjÃ¡n a kÃ©rdÃ©s nem vÃ¡laszolhatÃ³ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 3: polgÃ¡ri jog (easy)\n",
            "â“ Question: Mi a kÃ¼lÃ¶nbsÃ©g az alperes Ã©s a felperes kÃ¶zÃ¶tt?\n",
            "\n",
            "ğŸ“Š Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 1.000\n",
            "   MRR: 1.000\n",
            "   Relevant Found: 2/2\n",
            "   Retrieved Docs: ['Pf.20055/2020/5', 'P.20693/2011/64', 'Gfv.30261/2009/6', 'Mf.633718/2009/5', 'Pf.21465/2014/3']\n",
            "\n",
            "ğŸ“ Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: ['felperes', 'alperes', 'bÃ­rÃ³sÃ¡g', 'jog', 'per']\n",
            "   Has Structure: True\n",
            "   Has Citations: True (5 citations)\n",
            "   Word Count: 363\n",
            "   Hungarian Quality: 0.333\n",
            "   Semantic Similarity: 0.702\n",
            "\n",
            "ğŸ’¬ Generated Answer (first 300 chars):\n",
            "   ### **1. SzintetizÃ¡lt VÃ¡lasz**\n",
            "\n",
            "A rendelkezÃ©sre Ã¡llÃ³ dokumentumok alapjÃ¡n a felperes az a fÃ©l, aki a perben valamilyen igÃ©nyt vagy kereseti kÃ©relmet Ã©rvÃ©nyesÃ­t, mÃ­g az alperes az a fÃ©l, akivel szemben a felperes ezt az igÃ©nyt tÃ¡masztja, Ã©s akit helytÃ¡llÃ¡si kÃ¶telezettsÃ©g terhelhet. A felperesnek kell...\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 4: bÃ¼ntetÅ‘jog (medium)\n",
            "â“ Question: Mikor alkalmazhatÃ³ a feltÃ©teles szabadsÃ¡g?\n",
            "\n",
            "ğŸ“Š Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 0.667\n",
            "   MRR: 0.500\n",
            "   Relevant Found: 2/3\n",
            "   Retrieved Docs: ['P.20693/2011/64', 'Bf.38/2009/37', 'B.101/2021/25', 'P.20350/2015/3', 'G.40032/2018/12']\n",
            "\n",
            "ğŸ“ Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.615\n",
            "\n",
            "ğŸ’¬ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapjÃ¡n a kÃ©rdÃ©s nem vÃ¡laszolhatÃ³ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 5: polgÃ¡ri jog (hard)\n",
            "â“ Question: Mit jelent a bizonyÃ­tÃ¡si teher a polgÃ¡ri perben?\n",
            "\n",
            "ğŸ“Š Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 1.000\n",
            "   MRR: 0.333\n",
            "   Relevant Found: 2/2\n",
            "   Retrieved Docs: ['Gf.20329/2013/4', 'Gf.20216/2013/4', 'Pf.20156/2010/5', 'B.101/2021/25', 'G.40289/2007/29']\n",
            "\n",
            "ğŸ“ Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.557\n",
            "\n",
            "ğŸ’¬ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapjÃ¡n a kÃ©rdÃ©s nem vÃ¡laszolhatÃ³ meg....\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Detailed case-by-case analysis\n",
        "print(f\"\\nğŸ“‹ DETAILED CASE-BY-CASE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in evaluation_results:\n",
        "    print(f\"\\nğŸ” Test Case {result['test_case']}: {result['domain']} ({result['difficulty']})\")\n",
        "    print(f\"â“ Question: {result['question']}\")\n",
        "    \n",
        "    # Retrieval analysis\n",
        "    ret_metrics = result['retrieval_metrics']\n",
        "    print(f\"\\nğŸ“Š Retrieval Performance:\")\n",
        "    print(f\"   Precision@5: {ret_metrics['precision_at_k']:.3f}\")\n",
        "    print(f\"   Recall@5: {ret_metrics['recall_at_k']:.3f}\")\n",
        "    print(f\"   MRR: {ret_metrics['mrr']:.3f}\")\n",
        "    print(f\"   Relevant Found: {ret_metrics['relevant_found']}/{ret_metrics['total_relevant']}\")\n",
        "    print(f\"   Retrieved Docs: {result['retrieved_docs']}\")\n",
        "    \n",
        "    # Answer analysis\n",
        "    ans_metrics = result['answer_metrics']\n",
        "    print(f\"\\nğŸ“ Answer Quality:\")\n",
        "    print(f\"   Key Points Coverage: {ans_metrics['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Found Key Points: {ans_metrics['key_points_found']}\")\n",
        "    print(f\"   Legal Terms Used: {ans_metrics['legal_terms_used']}\")\n",
        "    print(f\"   Has Structure: {ans_metrics['has_structured_response']}\")\n",
        "    print(f\"   Has Citations: {ans_metrics['has_citations']} ({ans_metrics['citation_count']} citations)\")\n",
        "    print(f\"   Word Count: {ans_metrics['word_count']}\")\n",
        "    print(f\"   Hungarian Quality: {ans_metrics['hungarian_quality_score']:.3f}\")\n",
        "    print(f\"   Semantic Similarity: {result['semantic_similarity']:.3f}\")\n",
        "    \n",
        "    # Answer preview\n",
        "    print(f\"\\nğŸ’¬ Generated Answer (first 300 chars):\")\n",
        "    print(f\"   {result['answer'][:300]}...\")\n",
        "    \n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Evaluation results saved to: data/evaluation_results.json\n",
            "ğŸ“Š Evaluation summary saved to: data/evaluation_summary.csv\n",
            "\n",
            "ğŸ¯ Use these files to track RAG system improvements over time!\n",
            "\n",
            "ğŸ“‹ EVALUATION SUMMARY TABLE:\n",
            "                                             question      domain difficulty  precision_at_5  key_points_coverage  semantic_similarity  has_citations  word_count\n",
            "         Mi a bÅ±nszervezet fogalma a Btk. szerint?...  bÃ¼ntetÅ‘jog     medium             0.2                  0.0             0.590658          False           9\n",
            "Milyen feltÃ©telei vannak a bÅ±nszervezetben valÃ³ rÃ©...  bÃ¼ntetÅ‘jog       hard             0.4                  0.0             0.622942          False           9\n",
            "   Mi a kÃ¼lÃ¶nbsÃ©g az alperes Ã©s a felperes kÃ¶zÃ¶tt?... polgÃ¡ri jog       easy             0.4                  0.0             0.702160           True         363\n",
            "        Mikor alkalmazhatÃ³ a feltÃ©teles szabadsÃ¡g?...  bÃ¼ntetÅ‘jog     medium             0.4                  0.0             0.615080          False           9\n",
            "  Mit jelent a bizonyÃ­tÃ¡si teher a polgÃ¡ri perben?... polgÃ¡ri jog       hard             0.4                  0.0             0.556559          False           9\n"
          ]
        }
      ],
      "source": [
        "# Save evaluation results for future reference\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Prepare results for JSON serialization\n",
        "serializable_results = []\n",
        "for result in evaluation_results:\n",
        "    serializable_result = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'test_case': result['test_case'],\n",
        "        'question': result['question'],\n",
        "        'domain': result['domain'],\n",
        "        'difficulty': result['difficulty'],\n",
        "        'answer': result['answer'],\n",
        "        'retrieval_metrics': result['retrieval_metrics'],\n",
        "        'answer_metrics': {\n",
        "            k: v for k, v in result['answer_metrics'].items() \n",
        "            if k not in ['key_points_found', 'legal_terms_used']  # Remove lists for JSON\n",
        "        },\n",
        "        'semantic_similarity': result['semantic_similarity'],\n",
        "        'retrieved_docs': result['retrieved_docs']\n",
        "    }\n",
        "    serializable_results.append(serializable_result)\n",
        "\n",
        "# Save to file\n",
        "output_file = \"data/evaluation_results.json\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"ğŸ’¾ Evaluation results saved to: {output_file}\")\n",
        "\n",
        "# Generate a quick CSV summary for easier analysis\n",
        "summary_data = []\n",
        "for result in evaluation_results:\n",
        "    summary_data.append({\n",
        "        'question': result['question'][:50] + \"...\",\n",
        "        'domain': result['domain'],\n",
        "        'difficulty': result['difficulty'],\n",
        "        'precision_at_5': result['retrieval_metrics']['precision_at_k'],\n",
        "        'key_points_coverage': result['answer_metrics']['key_points_coverage'],\n",
        "        'semantic_similarity': result['semantic_similarity'],\n",
        "        'has_citations': result['answer_metrics']['has_citations'],\n",
        "        'word_count': result['answer_metrics']['word_count']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_file = \"data/evaluation_summary.csv\"\n",
        "summary_df.to_csv(summary_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"ğŸ“Š Evaluation summary saved to: {summary_file}\")\n",
        "print(f\"\\nğŸ¯ Use these files to track RAG system improvements over time!\")\n",
        "\n",
        "# Display summary table\n",
        "print(f\"\\nğŸ“‹ EVALUATION SUMMARY TABLE:\")\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "legalqa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
