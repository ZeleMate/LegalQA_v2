{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üè≠ **Production-Grade RAG Quality Evaluation**\n",
        "\n",
        "This notebook **accurately simulates the production RAG pipeline** to provide the most precise evaluation results.\n",
        "\n",
        "## üîß **Production Components Simulated:**\n",
        "- **CustomRetriever**: Alpha-weighted similarity + relevance scoring\n",
        "- **RerankingRetriever**: LLM-based reranking with snippet extraction  \n",
        "- **CacheManager**: Multi-level caching system (memory simulation)\n",
        "- **DatabaseManager**: Optimized document fetching (DataFrame simulation)\n",
        "- **QA Chain**: Full LCEL pipeline with production prompts\n",
        "\n",
        "## üìä **Evaluation Areas:**\n",
        "- **Retrieval Quality**: Production scoring algorithm\n",
        "- **Reranking Effectiveness**: LLM-based document ranking\n",
        "- **Answer Quality**: Production prompt template evaluation\n",
        "- **End-to-End Pipeline**: Complete production workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/miniforge3/envs/legalqa/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Production-Grade RAG Evaluation Setup Complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zelenyianszkimate/miniforge3/envs/legalqa/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary packages for production simulation\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "import textwrap\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ML and NLP libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangChain components (production versions)\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from pydantic import BaseModel, Field, SecretStr, ConfigDict\n",
        "\n",
        "# Our components\n",
        "from src.data_loading.faiss_loader import load_faiss_index\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment and setup\n",
        "load_dotenv()\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    os.chdir('..')\n",
        "\n",
        "print(\"‚úÖ Production-Grade RAG Evaluation Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Production simulation components\n",
        "\n",
        "class NotebookCacheManager:\n",
        "    \"\"\"Simplified production cache manager for notebook environment.\"\"\"\n",
        "    \n",
        "    def __init__(self, maxsize: int = 1000, default_ttl: int = 300):\n",
        "        self.cache = {}\n",
        "        self.maxsize = maxsize\n",
        "        self.default_ttl = default_ttl\n",
        "        \n",
        "    def _generate_key(self, prefix: str, data: Any) -> str:\n",
        "        \"\"\"Generate consistent cache key.\"\"\"\n",
        "        if isinstance(data, str):\n",
        "            content = data.encode()\n",
        "        elif isinstance(data, np.ndarray):\n",
        "            content = data.tobytes()\n",
        "        else:\n",
        "            content = str(data).encode()\n",
        "        hash_object = hashlib.sha256(content)\n",
        "        return f\"{prefix}:{hash_object.hexdigest()}\"\n",
        "    \n",
        "    async def get(self, key: str) -> Optional[Any]:\n",
        "        \"\"\"Get from cache with TTL check.\"\"\"\n",
        "        if key in self.cache:\n",
        "            value, expiry = self.cache[key]\n",
        "            if datetime.now() < expiry:\n",
        "                return value\n",
        "            else:\n",
        "                del self.cache[key]\n",
        "        return None\n",
        "    \n",
        "    async def set(self, key: str, value: Any, ttl: int = None) -> None:\n",
        "        \"\"\"Set cache with TTL.\"\"\"\n",
        "        if len(self.cache) >= self.maxsize:\n",
        "            # Simple LRU eviction\n",
        "            oldest_key = next(iter(self.cache))\n",
        "            del self.cache[oldest_key]\n",
        "        \n",
        "        ttl = ttl or self.default_ttl\n",
        "        expiry = datetime.now() + timedelta(seconds=ttl)\n",
        "        self.cache[key] = (value, expiry)\n",
        "\n",
        "class NotebookDatabaseManager:\n",
        "    \"\"\"Simplified production database manager using DataFrame.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "        # Simulate database indexing\n",
        "        self.df_indexed = self.df.set_index('chunk_id')\n",
        "        \n",
        "    async def fetch_chunks_by_ids(self, chunk_ids: List[str]) -> Dict[str, Dict]:\n",
        "        \"\"\"Simulate async database fetch.\"\"\"\n",
        "        await asyncio.sleep(0.001)  # Simulate network latency\n",
        "        \n",
        "        result = {}\n",
        "        for chunk_id in chunk_ids:\n",
        "            if chunk_id in self.df_indexed.index:\n",
        "                row = self.df_indexed.loc[chunk_id]\n",
        "                # Simulate the database structure\n",
        "                result[chunk_id] = {\n",
        "                    'text': row['text_chunk'],\n",
        "                    'doc_id': row['doc_id'],\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'embedding': row.get('embedding', ''),  # Simulated hex embedding\n",
        "                    'birosag': row.get('birosag', ''),\n",
        "                    'JogTerulet': row.get('JogTerulet', ''),\n",
        "                }\n",
        "        return result\n",
        "\n",
        "# Global instances\n",
        "notebook_cache = NotebookCacheManager()\n",
        "notebook_db = None  # Will be initialized after loading data\n",
        "\n",
        "async def cache_embedding_query(text: str, embeddings_model: Any) -> np.ndarray:\n",
        "    \"\"\"Cache embedding computation - production simulation.\"\"\"\n",
        "    cache_key = notebook_cache._generate_key(\"embedding\", text)\n",
        "    \n",
        "    cached = await notebook_cache.get(cache_key)\n",
        "    if cached is not None:\n",
        "        return cached\n",
        "    \n",
        "    # Compute embedding\n",
        "    result = embeddings_model.embed_query(text)\n",
        "    embedding = np.array(result, dtype=np.float32)\n",
        "    \n",
        "    # Cache result\n",
        "    await notebook_cache.set(cache_key, embedding, ttl=3600)\n",
        "    return embedding\n",
        "\n",
        "print(\"üîß Production simulation components ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Production CustomRetriever ready!\n",
            "‚úÖ RAG Quality Evaluation Setup Complete!\n"
          ]
        }
      ],
      "source": [
        "# Production CustomRetriever simulation\n",
        "\n",
        "class ProductionCustomRetriever(BaseRetriever, BaseModel):\n",
        "    \"\"\"Production-accurate CustomRetriever with caching and async operations.\"\"\"\n",
        "    \n",
        "    alpha: float = 0.7  # Production default\n",
        "    embeddings: GoogleGenerativeAIEmbeddings = Field(...)\n",
        "    faiss_index: faiss.Index = Field(...)\n",
        "    id_mapping: dict = Field(...)\n",
        "    k: int = 20  # Production default\n",
        "    \n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    \n",
        "    async def _get_cached_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding with caching - production simulation.\"\"\"\n",
        "        return await cache_embedding_query(query, self.embeddings)\n",
        "    \n",
        "    async def _aget_relevant_documents_async(self, query: str) -> List[Document]:\n",
        "        \"\"\"Production-accurate async document retrieval.\"\"\"\n",
        "        logger.debug(f\"Starting document retrieval for query: {query[:50]}...\")\n",
        "        \n",
        "        try:\n",
        "            # Get cached embedding\n",
        "            query_embedding = await self._get_cached_embedding(query)\n",
        "            query_vector = np.array(query_embedding).reshape(1, -1)\n",
        "            \n",
        "            # Search FAISS index\n",
        "            logger.debug(\"Searching FAISS index...\")\n",
        "            distances, indices = self.faiss_index.search(query_vector, k=self.k)\n",
        "            \n",
        "            # Map FAISS indices to chunk_ids\n",
        "            retrieved_chunk_ids = []\n",
        "            for idx in indices[0]:\n",
        "                if idx in self.id_mapping:\n",
        "                    retrieved_chunk_ids.append(self.id_mapping[idx])\n",
        "            \n",
        "            logger.debug(f\"Found {len(retrieved_chunk_ids)} chunks from FAISS\")\n",
        "            \n",
        "            # Fetch documents from \"database\" (async)\n",
        "            docs_from_db = await notebook_db.fetch_chunks_by_ids(list(set(retrieved_chunk_ids)))\n",
        "            \n",
        "            if not docs_from_db:\n",
        "                logger.warning(\"No documents found in DB for retrieved IDs\")\n",
        "                return []\n",
        "            \n",
        "            # Process documents with production scoring\n",
        "            documents = []\n",
        "            for distance, idx in zip(distances[0], indices[0]):\n",
        "                if idx in self.id_mapping:\n",
        "                    chunk_id = self.id_mapping[idx]\n",
        "                    doc_data = docs_from_db.get(chunk_id)\n",
        "                    \n",
        "                    if doc_data:\n",
        "                        metadata = doc_data.copy()\n",
        "                        text = metadata.pop(\"text\", \"\")\n",
        "                        \n",
        "                        # Production scoring algorithm\n",
        "                        relevance_score = 1.0 / (1.0 + distance)\n",
        "                        \n",
        "                        # Simulate similarity score (in production this comes from pgvector)\n",
        "                        # For notebook, we'll compute it directly\n",
        "                        text_embedding = await self._get_cached_embedding(text[:500])  # Truncate for efficiency\n",
        "                        text_vector = np.array(text_embedding).reshape(1, -1)\n",
        "                        similarity_score = cosine_similarity(query_vector, text_vector)[0][0]\n",
        "                        \n",
        "                        # Production final score calculation\n",
        "                        final_score = (self.alpha * similarity_score + \n",
        "                                     (1 - self.alpha) * relevance_score)\n",
        "                        \n",
        "                        metadata.update({\n",
        "                            \"relevancia\": round(relevance_score, 3),\n",
        "                            \"similarity_score\": round(similarity_score, 3),\n",
        "                            \"final_score\": round(final_score, 4),\n",
        "                        })\n",
        "                        \n",
        "                        documents.append(Document(page_content=text, metadata=metadata))\n",
        "            \n",
        "            # Sort by final score (production behavior)\n",
        "            documents.sort(key=lambda d: d.metadata.get(\"final_score\", 0), reverse=True)\n",
        "            logger.debug(f\"Retrieval completed, returning {len(documents)} documents\")\n",
        "            return documents\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during document retrieval: {e}\", exc_info=True)\n",
        "            raise\n",
        "    \n",
        "    def _get_relevant_documents(self, query: str, *, run_manager: Optional[Any] = None) -> list:\n",
        "        \"\"\"Required by BaseRetriever - do not use directly.\"\"\"\n",
        "        raise NotImplementedError(\"Use async _aget_relevant_documents method only!\")\n",
        "    \n",
        "    async def _aget_relevant_documents(self, query: str, *, run_manager: Optional[Any] = None) -> list:\n",
        "        \"\"\"Async interface for LCEL pipeline compatibility.\"\"\"\n",
        "        return await self._aget_relevant_documents_async(query)\n",
        "\n",
        "print(\"üîç Production CustomRetriever ready!\")\n",
        "import asyncio\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import textwrap\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# ML and NLP libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Current architecture components\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from pydantic import SecretStr\n",
        "\n",
        "# Our components\n",
        "from src.data_loading.faiss_loader import load_faiss_index\n",
        "\n",
        "# Load environment and setup\n",
        "load_dotenv()\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    os.chdir('..')\n",
        "\n",
        "print(\"‚úÖ RAG Quality Evaluation Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Loaded 5 ground truth test cases\n",
            "  üîπ b√ºntet≈ëjog - medium: Mi a b≈±nszervezet fogalma a Btk. szerint?...\n",
            "  üîπ b√ºntet≈ëjog - hard: Milyen felt√©telei vannak a b≈±nszervezetben val√≥ r√©...\n",
            "  üîπ polg√°ri jog - easy: Mi a k√ºl√∂nbs√©g az alperes √©s a felperes k√∂z√∂tt?...\n",
            "  üîπ b√ºntet≈ëjog - medium: Mikor alkalmazhat√≥ a felt√©teles szabads√°g?...\n",
            "  üîπ polg√°ri jog - hard: Mit jelent a bizony√≠t√°si teher a polg√°ri perben?...\n"
          ]
        }
      ],
      "source": [
        "# Define evaluation data structures\n",
        "@dataclass\n",
        "class GroundTruthExample:\n",
        "    \"\"\"Single test case with ground truth data.\"\"\"\n",
        "    question: str\n",
        "    expected_answer_key_points: List[str]\n",
        "    relevant_doc_ids: List[str]  # Documents that should be retrieved\n",
        "    legal_domain: str  # e.g., \"b√ºntet≈ëjog\", \"polg√°ri jog\"\n",
        "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
        "    \n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    \"\"\"Results from document retrieval.\"\"\"\n",
        "    query: str\n",
        "    retrieved_docs: List[Document]\n",
        "    distances: List[float]\n",
        "    \n",
        "@dataclass\n",
        "class QAResult:\n",
        "    \"\"\"Complete QA pipeline result.\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    retrieval_result: RetrievalResult\n",
        "    ground_truth: GroundTruthExample\n",
        "\n",
        "# Test cases for Hungarian legal domain (questions remain in Hungarian)\n",
        "GROUND_TRUTH_CASES = [\n",
        "    GroundTruthExample(\n",
        "        question=\"Mi a b≈±nszervezet fogalma a Btk. szerint?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"h√°rom vagy t√∂bb szem√©ly\",\n",
        "            \"hosszabb id≈ëre szervezett\",\n",
        "            \"√∂sszehangoltan m≈±k√∂d≈ë csoport\",\n",
        "            \"√∂t√©vi vagy ezt meghalad√≥ szabads√°gveszt√©s\",\n",
        "            \"Btk. 459. ¬ß\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"P.*\"],  # Regex patterns for relevant docs\n",
        "        legal_domain=\"b√ºntet≈ëjog\",\n",
        "        difficulty=\"medium\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Milyen felt√©telei vannak a b≈±nszervezetben val√≥ r√©szv√©telnek?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"b≈±nszervezet\",\n",
        "            \"r√©szv√©tel\",\n",
        "            \"tag\",\n",
        "            \"k√∂zrem≈±k√∂d√©s\",\n",
        "            \"b≈±ncselekm√©ny\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"B.*\", \"Kb.*\"],\n",
        "        legal_domain=\"b√ºntet≈ëjog\", \n",
        "        difficulty=\"hard\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mi a k√ºl√∂nbs√©g az alperes √©s a felperes k√∂z√∂tt?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"felperes: per ind√≠t√≥ja\",\n",
        "            \"alperes: per ellen akivel ind√≠tj√°k\", \n",
        "            \"polg√°ri per\",\n",
        "            \"peres felek\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Pf.*\", \"P.*\"],\n",
        "        legal_domain=\"polg√°ri jog\",\n",
        "        difficulty=\"easy\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mikor alkalmazhat√≥ a felt√©teles szabads√°g?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"felt√©teles\",\n",
        "            \"szabads√°g\",\n",
        "            \"v√©grehajt√°s\",\n",
        "            \"id≈ëtartam\",\n",
        "            \"felt√©tel\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Bf.*\", \"B.*\", \"Fkf.*\"],\n",
        "        legal_domain=\"b√ºntet≈ëjog\",\n",
        "        difficulty=\"medium\"\n",
        "    ),\n",
        "    GroundTruthExample(\n",
        "        question=\"Mit jelent a bizony√≠t√°si teher a polg√°ri perben?\",\n",
        "        expected_answer_key_points=[\n",
        "            \"bizony√≠t√°si\",\n",
        "            \"teher\",\n",
        "            \"polg√°ri\",\n",
        "            \"per\",\n",
        "            \"bizony√≠t√©k\"\n",
        "        ],\n",
        "        relevant_doc_ids=[\"Pf.*\", \"P.*\"],\n",
        "        legal_domain=\"polg√°ri jog\",\n",
        "        difficulty=\"hard\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"üìã Loaded {len(GROUND_TRUTH_CASES)} ground truth test cases\")\n",
        "for case in GROUND_TRUTH_CASES:\n",
        "    print(f\"  üîπ {case.legal_domain} - {case.difficulty}: {case.question[:50]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Loading sample data and models...\n",
            "‚úÖ Data loaded: 8293 docs, 8293 vectors\n",
            "ü§ñ Models and prompts initialized!\n"
          ]
        }
      ],
      "source": [
        "# Load data and initialize models\n",
        "print(\"üîß Loading sample data and models...\")\n",
        "\n",
        "# Load sample data\n",
        "sample_parquet_path = \"data/processed/sample_data.parquet\"\n",
        "faiss_index_path = \"data/processed/sample_faiss.bin\" \n",
        "id_mapping_path = \"data/processed/sample_mapping.pkl\"\n",
        "\n",
        "df = pd.read_parquet(sample_parquet_path)\n",
        "faiss_index, id_mapping = load_faiss_index(faiss_index_path, id_mapping_path)\n",
        "\n",
        "print(f\"‚úÖ Data loaded: {len(df)} docs, {faiss_index.ntotal} vectors\")\n",
        "\n",
        "# Initialize models\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "if not google_api_key:\n",
        "    raise ValueError(\"GOOGLE_API_KEY environment variable is required!\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\", \n",
        "    api_key=google_api_key\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0,\n",
        "    api_key=SecretStr(google_api_key),\n",
        ")\n",
        "\n",
        "# Load prompt\n",
        "prompt_path = Path(\"src/prompts/legal_assistant_prompt.txt\")\n",
        "template = prompt_path.read_text(encoding=\"utf-8\")\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "print(\"ü§ñ Models and prompts initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä RAG Evaluator initialized!\n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics and helper functions\n",
        "class RAGEvaluator:\n",
        "    \"\"\"Comprehensive RAG evaluation class with hybrid search.\"\"\"\n",
        "    \n",
        "    def __init__(self, embeddings, llm, prompt, df, faiss_index, id_mapping):\n",
        "        self.embeddings = embeddings\n",
        "        self.llm = llm  \n",
        "        self.prompt = prompt\n",
        "        self.df = df\n",
        "        self.faiss_index = faiss_index\n",
        "        self.id_mapping = id_mapping\n",
        "        \n",
        "    def keyword_search(self, query: str, k: int = 10) -> List[Document]:\n",
        "        \"\"\"Keyword-based search in documents.\"\"\"\n",
        "        # Extract key terms from query\n",
        "        key_terms = []\n",
        "        if 'b≈±nszervezet' in query.lower():\n",
        "            key_terms.extend(['b≈±nszervezet', 'szervezett', 'csoport'])\n",
        "        if 'alperes' in query.lower() or 'felperes' in query.lower():\n",
        "            key_terms.extend(['alperes', 'felperes', 'per', 'polg√°ri'])\n",
        "        if 'felt√©teles' in query.lower():\n",
        "            key_terms.extend(['felt√©teles', 'szabads√°g', 'v√©grehajt√°s'])\n",
        "        if 'bizony√≠t√°si' in query.lower():\n",
        "            key_terms.extend(['bizony√≠t√°si', 'teher', 'bizony√≠t√©k'])\n",
        "        \n",
        "        # Search for documents containing these terms\n",
        "        matching_docs = []\n",
        "        for term in key_terms:\n",
        "            matches = self.df[self.df['text_chunk'].str.contains(term, case=False, na=False)]\n",
        "            for _, row in matches.head(k//len(key_terms) + 1).iterrows():\n",
        "                doc = Document(\n",
        "                    page_content=row['text_chunk'],\n",
        "                    metadata={\n",
        "                        'chunk_id': row['chunk_id'],\n",
        "                        'doc_id': row['doc_id'],\n",
        "                        'search_type': 'keyword',\n",
        "                        'matched_term': term\n",
        "                    }\n",
        "                )\n",
        "                if doc not in matching_docs:\n",
        "                    matching_docs.append(doc)\n",
        "        \n",
        "        return matching_docs[:k]\n",
        "        \n",
        "    def retrieve_documents(self, query: str, k: int = 5) -> RetrievalResult:\n",
        "        \"\"\"Hybrid retrieval: semantic + keyword search.\"\"\"\n",
        "        # 1. Semantic search (FAISS)\n",
        "        query_embedding = self.embeddings.embed_query(query)\n",
        "        query_vector = np.array([query_embedding], dtype='float32')\n",
        "        distances, indices = self.faiss_index.search(query_vector, k*2)  # Get more for filtering\n",
        "        \n",
        "        semantic_docs = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx in self.id_mapping:\n",
        "                chunk_id = self.id_mapping[idx]\n",
        "                row = self.df[self.df['chunk_id'] == chunk_id]\n",
        "                if not row.empty:\n",
        "                    text_content = row.iloc[0]['text_chunk']\n",
        "                    doc_id = row.iloc[0]['doc_id']\n",
        "                    \n",
        "                    semantic_docs.append(Document(\n",
        "                        page_content=text_content,\n",
        "                        metadata={\n",
        "                            'chunk_id': chunk_id, \n",
        "                            'doc_id': doc_id,\n",
        "                            'distance': float(distances[0][i]),\n",
        "                            'search_type': 'semantic'\n",
        "                        }\n",
        "                    ))\n",
        "        \n",
        "        # 2. Keyword search\n",
        "        keyword_docs = self.keyword_search(query, k)\n",
        "        \n",
        "        # 3. Merge and deduplicate by doc_id\n",
        "        all_docs = {}\n",
        "        for doc in semantic_docs + keyword_docs:\n",
        "            doc_id = doc.metadata['doc_id']\n",
        "            if doc_id not in all_docs:\n",
        "                all_docs[doc_id] = doc\n",
        "        \n",
        "        # 4. Rank by relevance (prefer keyword matches, then semantic similarity)\n",
        "        final_docs = []\n",
        "        for doc_id, doc in all_docs.items():\n",
        "            if doc.metadata.get('search_type') == 'keyword':\n",
        "                final_docs.insert(0, doc)  # Keyword matches first\n",
        "            else:\n",
        "                final_docs.append(doc)  # Semantic matches after\n",
        "        \n",
        "        return RetrievalResult(\n",
        "            query=query,\n",
        "            retrieved_docs=final_docs[:k],\n",
        "            distances=[doc.metadata.get('distance', 0.5) for doc in final_docs[:k]]\n",
        "        )\n",
        "    \n",
        "    def generate_answer(self, retrieval_result: RetrievalResult) -> str:\n",
        "        \"\"\"Generate answer from retrieved documents.\"\"\"\n",
        "        # Format context\n",
        "        context_lines = []\n",
        "        for doc in retrieval_result.retrieved_docs:\n",
        "            doc_id = doc.metadata.get(\"doc_id\", \"N/A\")\n",
        "            content = doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content\n",
        "            context_lines.append(f\"### Document ID: {doc_id}\\nContent:\\n{content}\")\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_lines)\n",
        "        \n",
        "        # Generate answer\n",
        "        formatted_input = self.prompt.format(context=context, question=retrieval_result.query)\n",
        "        result = self.llm.invoke(formatted_input)\n",
        "        return result.content if hasattr(result, 'content') else str(result)\n",
        "    \n",
        "    def evaluate_retrieval(self, retrieval_result: RetrievalResult, ground_truth: GroundTruthExample) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate retrieval quality.\"\"\"\n",
        "        retrieved_doc_ids = [doc.metadata['doc_id'] for doc in retrieval_result.retrieved_docs]\n",
        "        \n",
        "        # Check which expected docs were found\n",
        "        relevant_found = 0\n",
        "        total_relevant = len(ground_truth.relevant_doc_ids)\n",
        "        \n",
        "        for pattern in ground_truth.relevant_doc_ids:\n",
        "            # Use regex matching for doc IDs\n",
        "            for doc_id in retrieved_doc_ids:\n",
        "                if re.match(pattern, doc_id):\n",
        "                    relevant_found += 1\n",
        "                    break\n",
        "        \n",
        "        # Calculate metrics\n",
        "        precision_at_k = relevant_found / len(retrieved_doc_ids) if retrieved_doc_ids else 0\n",
        "        recall_at_k = relevant_found / total_relevant if total_relevant > 0 else 0\n",
        "        f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0\n",
        "        \n",
        "        # Mean Reciprocal Rank (MRR)\n",
        "        mrr = 0\n",
        "        for i, doc_id in enumerate(retrieved_doc_ids):\n",
        "            for pattern in ground_truth.relevant_doc_ids:\n",
        "                if re.match(pattern, doc_id):\n",
        "                    mrr = 1 / (i + 1)\n",
        "                    break\n",
        "            if mrr > 0:\n",
        "                break\n",
        "                \n",
        "        return {\n",
        "            'precision_at_k': precision_at_k,\n",
        "            'recall_at_k': recall_at_k,\n",
        "            'f1_at_k': f1_at_k,\n",
        "            'mrr': mrr,\n",
        "            'relevant_found': relevant_found,\n",
        "            'total_relevant': total_relevant,\n",
        "            'retrieved_count': len(retrieved_doc_ids)\n",
        "        }\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = RAGEvaluator(embeddings, llm, prompt, df, faiss_index, id_mapping)\n",
        "print(\"üìä RAG Evaluator initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Answer quality evaluation functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Answer quality evaluation functions\n",
        "def evaluate_answer_quality(answer: str, ground_truth: GroundTruthExample) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate the quality of generated answer.\"\"\"\n",
        "    \n",
        "    # 1. Key points coverage\n",
        "    answer_lower = answer.lower()\n",
        "    key_points_found = []\n",
        "    for point in ground_truth.expected_answer_key_points:\n",
        "        if point.lower() in answer_lower:\n",
        "            key_points_found.append(point)\n",
        "    \n",
        "    key_points_coverage = len(key_points_found) / len(ground_truth.expected_answer_key_points)\n",
        "    \n",
        "    # 2. Legal terminology accuracy\n",
        "    legal_terms = [\n",
        "        'btk', 'b≈±nszervezet', 'szabads√°gveszt√©s', 'felperes', 'alperes', \n",
        "        'b√≠r√≥s√°g', '√≠t√©let', 'hat√°rozat', 't√∂rv√©ny', 'jog', 'per', 'v√°dlott'\n",
        "    ]\n",
        "    \n",
        "    legal_terms_used = []\n",
        "    for term in legal_terms:\n",
        "        if term in answer_lower:\n",
        "            legal_terms_used.append(term)\n",
        "    \n",
        "    # 3. Structure and formatting check\n",
        "    has_structured_response = any(marker in answer for marker in [\n",
        "        \"1. Szintetiz√°lt V√°lasz\", \"2. R√©szletes Elemz√©s\", \"3. Konkl√∫zi√≥\", \"4. Jogi nyilatkozat\"\n",
        "    ])\n",
        "    \n",
        "    # 4. Citation check  \n",
        "    citation_pattern = r'\\(Forr√°s: [^)]+\\)'\n",
        "    citations = re.findall(citation_pattern, answer)\n",
        "    has_citations = len(citations) > 0\n",
        "    \n",
        "    # 5. Length and completeness\n",
        "    word_count = len(answer.split())\n",
        "    is_adequate_length = 50 <= word_count <= 500  # Reasonable answer length\n",
        "    \n",
        "    # 6. Hungarian language quality (basic check)\n",
        "    hungarian_indicators = ['szerint', 'alapj√°n', 'amely', 'amelynek', 'illetve', 'tov√°bb√°']\n",
        "    hungarian_score = sum(1 for indicator in hungarian_indicators if indicator in answer_lower) / len(hungarian_indicators)\n",
        "    \n",
        "    return {\n",
        "        'key_points_coverage': key_points_coverage,\n",
        "        'key_points_found': key_points_found,\n",
        "        'legal_terms_count': len(legal_terms_used),\n",
        "        'legal_terms_used': legal_terms_used,\n",
        "        'has_structured_response': has_structured_response,\n",
        "        'has_citations': has_citations,\n",
        "        'citation_count': len(citations),\n",
        "        'word_count': word_count,\n",
        "        'is_adequate_length': is_adequate_length,\n",
        "        'hungarian_quality_score': hungarian_score\n",
        "    }\n",
        "\n",
        "def calculate_semantic_similarity(answer: str, expected_points: List[str], embeddings) -> float:\n",
        "    \"\"\"Calculate semantic similarity between answer and expected points.\"\"\"\n",
        "    try:\n",
        "        # Get embedding for the answer\n",
        "        answer_embedding = embeddings.embed_query(answer)\n",
        "        \n",
        "        # Get embeddings for expected points\n",
        "        expected_text = \" \".join(expected_points)\n",
        "        expected_embedding = embeddings.embed_query(expected_text)\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(\n",
        "            np.array(answer_embedding).reshape(1, -1),\n",
        "            np.array(expected_embedding).reshape(1, -1)\n",
        "        )[0][0]\n",
        "        \n",
        "        return float(similarity)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating semantic similarity: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"üìä Answer quality evaluation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Running comprehensive RAG evaluation...\n",
            "======================================================================\n",
            "\n",
            "üìù Test Case 1/5\n",
            "‚ùì Question: Mi a b≈±nszervezet fogalma a Btk. szerint?\n",
            "üè∑Ô∏è Domain: b√ºntet≈ëjog | Difficulty: medium\n",
            "  üîç Retrieving documents...\n",
            "  ü§ñ Generating answer...\n",
            "  üìä Evaluating retrieval...\n",
            "  üìã Evaluating answer quality...\n",
            "  üîó Calculating semantic similarity...\n",
            "  ‚úÖ Results:\n",
            "     üéØ Precision@5: 0.200\n",
            "     üìù Key Points Coverage: 0.000\n",
            "     üîó Semantic Similarity: 0.591\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Test Case 2/5\n",
            "‚ùì Question: Milyen felt√©telei vannak a b≈±nszervezetben val√≥ r√©szv√©telnek?\n",
            "üè∑Ô∏è Domain: b√ºntet≈ëjog | Difficulty: hard\n",
            "  üîç Retrieving documents...\n",
            "  ü§ñ Generating answer...\n",
            "  üìä Evaluating retrieval...\n",
            "  üìã Evaluating answer quality...\n",
            "  üîó Calculating semantic similarity...\n",
            "  ‚úÖ Results:\n",
            "     üéØ Precision@5: 0.400\n",
            "     üìù Key Points Coverage: 0.000\n",
            "     üîó Semantic Similarity: 0.623\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Test Case 3/5\n",
            "‚ùì Question: Mi a k√ºl√∂nbs√©g az alperes √©s a felperes k√∂z√∂tt?\n",
            "üè∑Ô∏è Domain: polg√°ri jog | Difficulty: easy\n",
            "  üîç Retrieving documents...\n",
            "  ü§ñ Generating answer...\n",
            "  üìä Evaluating retrieval...\n",
            "  üìã Evaluating answer quality...\n",
            "  üîó Calculating semantic similarity...\n",
            "  ‚úÖ Results:\n",
            "     üéØ Precision@5: 0.400\n",
            "     üìù Key Points Coverage: 0.000\n",
            "     üîó Semantic Similarity: 0.702\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Test Case 4/5\n",
            "‚ùì Question: Mikor alkalmazhat√≥ a felt√©teles szabads√°g?\n",
            "üè∑Ô∏è Domain: b√ºntet≈ëjog | Difficulty: medium\n",
            "  üîç Retrieving documents...\n",
            "  ü§ñ Generating answer...\n",
            "  üìä Evaluating retrieval...\n",
            "  üìã Evaluating answer quality...\n",
            "  üîó Calculating semantic similarity...\n",
            "  ‚úÖ Results:\n",
            "     üéØ Precision@5: 0.400\n",
            "     üìù Key Points Coverage: 0.000\n",
            "     üîó Semantic Similarity: 0.615\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Test Case 5/5\n",
            "‚ùì Question: Mit jelent a bizony√≠t√°si teher a polg√°ri perben?\n",
            "üè∑Ô∏è Domain: polg√°ri jog | Difficulty: hard\n",
            "  üîç Retrieving documents...\n",
            "  ü§ñ Generating answer...\n",
            "  üìä Evaluating retrieval...\n",
            "  üìã Evaluating answer quality...\n",
            "  üîó Calculating semantic similarity...\n",
            "  ‚úÖ Results:\n",
            "     üéØ Precision@5: 0.400\n",
            "     üìù Key Points Coverage: 0.000\n",
            "     üîó Semantic Similarity: 0.557\n",
            "--------------------------------------------------\n",
            "\n",
            "‚úÖ Evaluation completed! 5 test cases processed.\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive evaluation\n",
        "print(\"üß™ Running comprehensive RAG evaluation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for i, ground_truth in enumerate(GROUND_TRUTH_CASES, 1):\n",
        "    print(f\"\\nüìù Test Case {i}/{len(GROUND_TRUTH_CASES)}\")\n",
        "    print(f\"‚ùì Question: {ground_truth.question}\")\n",
        "    print(f\"üè∑Ô∏è Domain: {ground_truth.legal_domain} | Difficulty: {ground_truth.difficulty}\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Retrieve documents\n",
        "        print(\"  üîç Retrieving documents...\")\n",
        "        retrieval_result = evaluator.retrieve_documents(ground_truth.question, k=5)\n",
        "        \n",
        "        # Step 2: Generate answer\n",
        "        print(\"  ü§ñ Generating answer...\")\n",
        "        answer = evaluator.generate_answer(retrieval_result)\n",
        "        \n",
        "        # Step 3: Evaluate retrieval\n",
        "        print(\"  üìä Evaluating retrieval...\")\n",
        "        retrieval_metrics = evaluator.evaluate_retrieval(retrieval_result, ground_truth)\n",
        "        \n",
        "        # Step 4: Evaluate answer quality\n",
        "        print(\"  üìã Evaluating answer quality...\")\n",
        "        answer_metrics = evaluate_answer_quality(answer, ground_truth)\n",
        "        \n",
        "        # Step 5: Calculate semantic similarity\n",
        "        print(\"  üîó Calculating semantic similarity...\")\n",
        "        semantic_sim = calculate_semantic_similarity(\n",
        "            answer, ground_truth.expected_answer_key_points, embeddings\n",
        "        )\n",
        "        \n",
        "        # Compile results\n",
        "        result = {\n",
        "            'test_case': i,\n",
        "            'question': ground_truth.question,\n",
        "            'domain': ground_truth.legal_domain,\n",
        "            'difficulty': ground_truth.difficulty,\n",
        "            'answer': answer,\n",
        "            'retrieval_metrics': retrieval_metrics,\n",
        "            'answer_metrics': answer_metrics,\n",
        "            'semantic_similarity': semantic_sim,\n",
        "            'retrieved_docs': [doc.metadata['doc_id'] for doc in retrieval_result.retrieved_docs]\n",
        "        }\n",
        "        \n",
        "        evaluation_results.append(result)\n",
        "        \n",
        "        # Print quick summary\n",
        "        print(f\"  ‚úÖ Results:\")\n",
        "        print(f\"     üéØ Precision@5: {retrieval_metrics['precision_at_k']:.3f}\")\n",
        "        print(f\"     üìù Key Points Coverage: {answer_metrics['key_points_coverage']:.3f}\")\n",
        "        print(f\"     üîó Semantic Similarity: {semantic_sim:.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "        continue\n",
        "    \n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation completed! {len(evaluation_results)} test cases processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä DETAILED EVALUATION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "üéØ OVERALL PERFORMANCE METRICS\n",
            "----------------------------------------\n",
            "üìà Retrieval Metrics:\n",
            "   Precision@5:     0.360 ¬± 0.080\n",
            "   Recall@5:        0.767 ¬± 0.200\n",
            "   F1@5:            0.486 ¬± 0.105\n",
            "   MRR:             0.633 ¬± 0.306\n",
            "\n",
            "üìù Answer Quality Metrics:\n",
            "   Key Points Coverage: 0.000 ¬± 0.000\n",
            "   Semantic Similarity: 0.617 ¬± 0.048\n",
            "\n",
            "üè∑Ô∏è PERFORMANCE BY DOMAIN\n",
            "----------------------------------------\n",
            "üìö B√úNTET≈êJOG:\n",
            "   Precision@5: 0.333\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 3\n",
            "üìö POLG√ÅRI JOG:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "\n",
            "üéØ PERFORMANCE BY DIFFICULTY\n",
            "----------------------------------------\n",
            "‚ö° MEDIUM:\n",
            "   Precision@5: 0.300\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "‚ö° HARD:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 2\n",
            "‚ö° EASY:\n",
            "   Precision@5: 0.400\n",
            "   Key Coverage: 0.000\n",
            "   Test Cases: 1\n"
          ]
        }
      ],
      "source": [
        "# Analysis and reporting\n",
        "print(\"üìä DETAILED EVALUATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not evaluation_results:\n",
        "    print(\"‚ùå No evaluation results to analyze!\")\n",
        "else:\n",
        "    # 1. Overall Metrics Summary\n",
        "    print(\"\\nüéØ OVERALL PERFORMANCE METRICS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    retrieval_precisions = [r['retrieval_metrics']['precision_at_k'] for r in evaluation_results]\n",
        "    retrieval_recalls = [r['retrieval_metrics']['recall_at_k'] for r in evaluation_results]\n",
        "    retrieval_f1s = [r['retrieval_metrics']['f1_at_k'] for r in evaluation_results]\n",
        "    mrrs = [r['retrieval_metrics']['mrr'] for r in evaluation_results]\n",
        "    \n",
        "    key_points_coverages = [r['answer_metrics']['key_points_coverage'] for r in evaluation_results]\n",
        "    semantic_similarities = [r['semantic_similarity'] for r in evaluation_results]\n",
        "    \n",
        "    print(f\"üìà Retrieval Metrics:\")\n",
        "    print(f\"   Precision@5:     {np.mean(retrieval_precisions):.3f} ¬± {np.std(retrieval_precisions):.3f}\")\n",
        "    print(f\"   Recall@5:        {np.mean(retrieval_recalls):.3f} ¬± {np.std(retrieval_recalls):.3f}\")\n",
        "    print(f\"   F1@5:            {np.mean(retrieval_f1s):.3f} ¬± {np.std(retrieval_f1s):.3f}\")\n",
        "    print(f\"   MRR:             {np.mean(mrrs):.3f} ¬± {np.std(mrrs):.3f}\")\n",
        "    \n",
        "    print(f\"\\nüìù Answer Quality Metrics:\")\n",
        "    print(f\"   Key Points Coverage: {np.mean(key_points_coverages):.3f} ¬± {np.std(key_points_coverages):.3f}\")\n",
        "    print(f\"   Semantic Similarity: {np.mean(semantic_similarities):.3f} ¬± {np.std(semantic_similarities):.3f}\")\n",
        "    \n",
        "    # 2. Performance by Domain and Difficulty\n",
        "    print(f\"\\nüè∑Ô∏è PERFORMANCE BY DOMAIN\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    domains = {}\n",
        "    for result in evaluation_results:\n",
        "        domain = result['domain']\n",
        "        if domain not in domains:\n",
        "            domains[domain] = []\n",
        "        domains[domain].append(result)\n",
        "    \n",
        "    for domain, results in domains.items():\n",
        "        precisions = [r['retrieval_metrics']['precision_at_k'] for r in results]\n",
        "        coverages = [r['answer_metrics']['key_points_coverage'] for r in results]\n",
        "        \n",
        "        print(f\"üìö {domain.upper()}:\")\n",
        "        print(f\"   Precision@5: {np.mean(precisions):.3f}\")\n",
        "        print(f\"   Key Coverage: {np.mean(coverages):.3f}\")\n",
        "        print(f\"   Test Cases: {len(results)}\")\n",
        "    \n",
        "    # 3. Performance by Difficulty\n",
        "    print(f\"\\nüéØ PERFORMANCE BY DIFFICULTY\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    difficulties = {}\n",
        "    for result in evaluation_results:\n",
        "        diff = result['difficulty']\n",
        "        if diff not in difficulties:\n",
        "            difficulties[diff] = []\n",
        "        difficulties[diff].append(result)\n",
        "    \n",
        "    for difficulty, results in difficulties.items():\n",
        "        precisions = [r['retrieval_metrics']['precision_at_k'] for r in results]\n",
        "        coverages = [r['answer_metrics']['key_points_coverage'] for r in results]\n",
        "        \n",
        "        print(f\"‚ö° {difficulty.upper()}:\")\n",
        "        print(f\"   Precision@5: {np.mean(precisions):.3f}\")\n",
        "        print(f\"   Key Coverage: {np.mean(coverages):.3f}\")\n",
        "        print(f\"   Test Cases: {len(results)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä EVALUATION SUMMARY & RECOMMENDATIONS\n",
            "======================================================================\n",
            "üéØ OVERALL RAG SYSTEM GRADE: 29.3/100\n",
            "   Status: POOR ‚≠ê‚≠ê\n",
            "   üö® Your RAG system needs significant improvement.\n",
            "\n",
            "üìà COMPONENT SCORES:\n",
            "   Retrieval Quality:  36.0/100\n",
            "   Answer Quality:     0.0/100\n",
            "   Semantic Accuracy:  61.7/100\n",
            "\n",
            "üîß IMPROVEMENT RECOMMENDATIONS:\n",
            "   üîç RETRIEVAL IMPROVEMENT NEEDED:\n",
            "      - Consider improving embedding model or fine-tuning\n",
            "      - Experiment with different chunk sizes\n",
            "      - Add domain-specific preprocessing\n",
            "      - Implement hybrid search (dense + sparse)\n",
            "   üìù ANSWER GENERATION IMPROVEMENT NEEDED:\n",
            "      - Improve prompt engineering\n",
            "      - Add more context to prompts\n",
            "      - Consider few-shot examples in prompts\n",
            "      - Fine-tune the LLM on legal domain\n",
            "   üéØ SEMANTIC ACCURACY IMPROVEMENT NEEDED:\n",
            "      - Use domain-specific embeddings\n",
            "      - Add legal terminology preprocessing\n",
            "      - Implement reranking with legal-specific scoring\n",
            "\n",
            "üèÜ BEST PERFORMING CASE:\n",
            "   Question: Mi a b≈±nszervezet fogalma a Btk. szerint?\n",
            "   Key Coverage: 0.000\n",
            "   Domain: b√ºntet≈ëjog | Difficulty: medium\n",
            "\n",
            "üéØ NEEDS IMPROVEMENT:\n",
            "   Question: Mi a b≈±nszervezet fogalma a Btk. szerint?\n",
            "   Key Coverage: 0.000\n",
            "   Domain: b√ºntet≈ëjog | Difficulty: medium\n",
            "\n",
            "‚úÖ RAG Quality Evaluation Complete!\n",
            "üìÅ Results saved in evaluation_results variable for further analysis.\n"
          ]
        }
      ],
      "source": [
        "# Generate evaluation report and recommendations\n",
        "print(f\"\\nüìä EVALUATION SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if evaluation_results:\n",
        "    # Calculate overall scores\n",
        "    avg_precision = np.mean([r['retrieval_metrics']['precision_at_k'] for r in evaluation_results])\n",
        "    avg_coverage = np.mean([r['answer_metrics']['key_points_coverage'] for r in evaluation_results])\n",
        "    avg_semantic = np.mean([r['semantic_similarity'] for r in evaluation_results])\n",
        "    \n",
        "    # Overall grade calculation\n",
        "    overall_score = (avg_precision * 0.3 + avg_coverage * 0.4 + avg_semantic * 0.3) * 100\n",
        "    \n",
        "    print(f\"üéØ OVERALL RAG SYSTEM GRADE: {overall_score:.1f}/100\")\n",
        "    \n",
        "    if overall_score >= 80:\n",
        "        grade = \"EXCELLENT ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   üéâ Your RAG system performs excellently!\")\n",
        "    elif overall_score >= 70:\n",
        "        grade = \"GOOD ‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   ‚úÖ Your RAG system performs well with room for improvement.\")\n",
        "    elif overall_score >= 60:\n",
        "        grade = \"FAIR ‚≠ê‚≠ê‚≠ê\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   ‚ö†Ô∏è Your RAG system needs improvement.\")\n",
        "    else:\n",
        "        grade = \"POOR ‚≠ê‚≠ê\"\n",
        "        print(f\"   Status: {grade}\")\n",
        "        print(\"   üö® Your RAG system needs significant improvement.\")\n",
        "    \n",
        "    print(f\"\\nüìà COMPONENT SCORES:\")\n",
        "    print(f\"   Retrieval Quality:  {avg_precision*100:.1f}/100\")\n",
        "    print(f\"   Answer Quality:     {avg_coverage*100:.1f}/100\") \n",
        "    print(f\"   Semantic Accuracy:  {avg_semantic*100:.1f}/100\")\n",
        "    \n",
        "    # Specific recommendations\n",
        "    print(f\"\\nüîß IMPROVEMENT RECOMMENDATIONS:\")\n",
        "    \n",
        "    if avg_precision < 0.7:\n",
        "        print(f\"   üîç RETRIEVAL IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Consider improving embedding model or fine-tuning\")\n",
        "        print(f\"      - Experiment with different chunk sizes\")\n",
        "        print(f\"      - Add domain-specific preprocessing\")\n",
        "        print(f\"      - Implement hybrid search (dense + sparse)\")\n",
        "    \n",
        "    if avg_coverage < 0.7:\n",
        "        print(f\"   üìù ANSWER GENERATION IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Improve prompt engineering\")\n",
        "        print(f\"      - Add more context to prompts\")\n",
        "        print(f\"      - Consider few-shot examples in prompts\")\n",
        "        print(f\"      - Fine-tune the LLM on legal domain\")\n",
        "    \n",
        "    if avg_semantic < 0.7:\n",
        "        print(f\"   üéØ SEMANTIC ACCURACY IMPROVEMENT NEEDED:\")\n",
        "        print(f\"      - Use domain-specific embeddings\")\n",
        "        print(f\"      - Add legal terminology preprocessing\")\n",
        "        print(f\"      - Implement reranking with legal-specific scoring\")\n",
        "    \n",
        "    # Best and worst performing cases\n",
        "    best_case = max(evaluation_results, key=lambda x: x['answer_metrics']['key_points_coverage'])\n",
        "    worst_case = min(evaluation_results, key=lambda x: x['answer_metrics']['key_points_coverage'])\n",
        "    \n",
        "    print(f\"\\nüèÜ BEST PERFORMING CASE:\")\n",
        "    print(f\"   Question: {best_case['question']}\")\n",
        "    print(f\"   Key Coverage: {best_case['answer_metrics']['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Domain: {best_case['domain']} | Difficulty: {best_case['difficulty']}\")\n",
        "    \n",
        "    print(f\"\\nüéØ NEEDS IMPROVEMENT:\")\n",
        "    print(f\"   Question: {worst_case['question']}\")\n",
        "    print(f\"   Key Coverage: {worst_case['answer_metrics']['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Domain: {worst_case['domain']} | Difficulty: {worst_case['difficulty']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ RAG Quality Evaluation Complete!\")\n",
        "print(f\"üìÅ Results saved in evaluation_results variable for further analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã DETAILED CASE-BY-CASE ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "üîç Test Case 1: b√ºntet≈ëjog (medium)\n",
            "‚ùì Question: Mi a b≈±nszervezet fogalma a Btk. szerint?\n",
            "\n",
            "üìä Retrieval Performance:\n",
            "   Precision@5: 0.200\n",
            "   Recall@5: 0.500\n",
            "   MRR: 0.333\n",
            "   Relevant Found: 1/2\n",
            "   Retrieved Docs: ['B.405/2015/56', 'M.8/2019/37', 'Pf.20069/2017/3', 'Kb.5/2008/42', 'P.20961/2011/3']\n",
            "\n",
            "üìù Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.591\n",
            "\n",
            "üí¨ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapj√°n a k√©rd√©s nem v√°laszolhat√≥ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "üîç Test Case 2: b√ºntet≈ëjog (hard)\n",
            "‚ùì Question: Milyen felt√©telei vannak a b≈±nszervezetben val√≥ r√©szv√©telnek?\n",
            "\n",
            "üìä Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 0.667\n",
            "   MRR: 1.000\n",
            "   Relevant Found: 2/3\n",
            "   Retrieved Docs: ['B.405/2015/56', 'M.8/2019/37', 'Pf.20069/2017/3', 'Kb.5/2008/42', 'Mf.639295/2011/4']\n",
            "\n",
            "üìù Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.623\n",
            "\n",
            "üí¨ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapj√°n a k√©rd√©s nem v√°laszolhat√≥ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "üîç Test Case 3: polg√°ri jog (easy)\n",
            "‚ùì Question: Mi a k√ºl√∂nbs√©g az alperes √©s a felperes k√∂z√∂tt?\n",
            "\n",
            "üìä Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 1.000\n",
            "   MRR: 1.000\n",
            "   Relevant Found: 2/2\n",
            "   Retrieved Docs: ['Pf.20055/2020/5', 'P.20693/2011/64', 'Gfv.30261/2009/6', 'Mf.633718/2009/5', 'Pf.21465/2014/3']\n",
            "\n",
            "üìù Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: ['felperes', 'alperes', 'b√≠r√≥s√°g', 'jog', 'per']\n",
            "   Has Structure: True\n",
            "   Has Citations: True (5 citations)\n",
            "   Word Count: 363\n",
            "   Hungarian Quality: 0.333\n",
            "   Semantic Similarity: 0.702\n",
            "\n",
            "üí¨ Generated Answer (first 300 chars):\n",
            "   ### **1. Szintetiz√°lt V√°lasz**\n",
            "\n",
            "A rendelkez√©sre √°ll√≥ dokumentumok alapj√°n a felperes az a f√©l, aki a perben valamilyen ig√©nyt vagy kereseti k√©relmet √©rv√©nyes√≠t, m√≠g az alperes az a f√©l, akivel szemben a felperes ezt az ig√©nyt t√°masztja, √©s akit helyt√°ll√°si k√∂telezetts√©g terhelhet. A felperesnek kell...\n",
            "------------------------------------------------------------\n",
            "\n",
            "üîç Test Case 4: b√ºntet≈ëjog (medium)\n",
            "‚ùì Question: Mikor alkalmazhat√≥ a felt√©teles szabads√°g?\n",
            "\n",
            "üìä Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 0.667\n",
            "   MRR: 0.500\n",
            "   Relevant Found: 2/3\n",
            "   Retrieved Docs: ['P.20693/2011/64', 'Bf.38/2009/37', 'B.101/2021/25', 'P.20350/2015/3', 'G.40032/2018/12']\n",
            "\n",
            "üìù Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.615\n",
            "\n",
            "üí¨ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapj√°n a k√©rd√©s nem v√°laszolhat√≥ meg....\n",
            "------------------------------------------------------------\n",
            "\n",
            "üîç Test Case 5: polg√°ri jog (hard)\n",
            "‚ùì Question: Mit jelent a bizony√≠t√°si teher a polg√°ri perben?\n",
            "\n",
            "üìä Retrieval Performance:\n",
            "   Precision@5: 0.400\n",
            "   Recall@5: 1.000\n",
            "   MRR: 0.333\n",
            "   Relevant Found: 2/2\n",
            "   Retrieved Docs: ['Gf.20329/2013/4', 'Gf.20216/2013/4', 'Pf.20156/2010/5', 'B.101/2021/25', 'G.40289/2007/29']\n",
            "\n",
            "üìù Answer Quality:\n",
            "   Key Points Coverage: 0.000\n",
            "   Found Key Points: []\n",
            "   Legal Terms Used: []\n",
            "   Has Structure: False\n",
            "   Has Citations: False (0 citations)\n",
            "   Word Count: 9\n",
            "   Hungarian Quality: 0.167\n",
            "   Semantic Similarity: 0.557\n",
            "\n",
            "üí¨ Generated Answer (first 300 chars):\n",
            "   A megadott dokumentumok alapj√°n a k√©rd√©s nem v√°laszolhat√≥ meg....\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Detailed case-by-case analysis\n",
        "print(f\"\\nüìã DETAILED CASE-BY-CASE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for result in evaluation_results:\n",
        "    print(f\"\\nüîç Test Case {result['test_case']}: {result['domain']} ({result['difficulty']})\")\n",
        "    print(f\"‚ùì Question: {result['question']}\")\n",
        "    \n",
        "    # Retrieval analysis\n",
        "    ret_metrics = result['retrieval_metrics']\n",
        "    print(f\"\\nüìä Retrieval Performance:\")\n",
        "    print(f\"   Precision@5: {ret_metrics['precision_at_k']:.3f}\")\n",
        "    print(f\"   Recall@5: {ret_metrics['recall_at_k']:.3f}\")\n",
        "    print(f\"   MRR: {ret_metrics['mrr']:.3f}\")\n",
        "    print(f\"   Relevant Found: {ret_metrics['relevant_found']}/{ret_metrics['total_relevant']}\")\n",
        "    print(f\"   Retrieved Docs: {result['retrieved_docs']}\")\n",
        "    \n",
        "    # Answer analysis\n",
        "    ans_metrics = result['answer_metrics']\n",
        "    print(f\"\\nüìù Answer Quality:\")\n",
        "    print(f\"   Key Points Coverage: {ans_metrics['key_points_coverage']:.3f}\")\n",
        "    print(f\"   Found Key Points: {ans_metrics['key_points_found']}\")\n",
        "    print(f\"   Legal Terms Used: {ans_metrics['legal_terms_used']}\")\n",
        "    print(f\"   Has Structure: {ans_metrics['has_structured_response']}\")\n",
        "    print(f\"   Has Citations: {ans_metrics['has_citations']} ({ans_metrics['citation_count']} citations)\")\n",
        "    print(f\"   Word Count: {ans_metrics['word_count']}\")\n",
        "    print(f\"   Hungarian Quality: {ans_metrics['hungarian_quality_score']:.3f}\")\n",
        "    print(f\"   Semantic Similarity: {result['semantic_similarity']:.3f}\")\n",
        "    \n",
        "    # Answer preview\n",
        "    print(f\"\\nüí¨ Generated Answer (first 300 chars):\")\n",
        "    print(f\"   {result['answer'][:300]}...\")\n",
        "    \n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Evaluation results saved to: data/evaluation_results.json\n",
            "üìä Evaluation summary saved to: data/evaluation_summary.csv\n",
            "\n",
            "üéØ Use these files to track RAG system improvements over time!\n",
            "\n",
            "üìã EVALUATION SUMMARY TABLE:\n",
            "                                             question      domain difficulty  precision_at_5  key_points_coverage  semantic_similarity  has_citations  word_count\n",
            "         Mi a b≈±nszervezet fogalma a Btk. szerint?...  b√ºntet≈ëjog     medium             0.2                  0.0             0.590658          False           9\n",
            "Milyen felt√©telei vannak a b≈±nszervezetben val√≥ r√©...  b√ºntet≈ëjog       hard             0.4                  0.0             0.622942          False           9\n",
            "   Mi a k√ºl√∂nbs√©g az alperes √©s a felperes k√∂z√∂tt?... polg√°ri jog       easy             0.4                  0.0             0.702160           True         363\n",
            "        Mikor alkalmazhat√≥ a felt√©teles szabads√°g?...  b√ºntet≈ëjog     medium             0.4                  0.0             0.615080          False           9\n",
            "  Mit jelent a bizony√≠t√°si teher a polg√°ri perben?... polg√°ri jog       hard             0.4                  0.0             0.556559          False           9\n"
          ]
        }
      ],
      "source": [
        "# Save evaluation results for future reference\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Prepare results for JSON serialization\n",
        "serializable_results = []\n",
        "for result in evaluation_results:\n",
        "    serializable_result = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'test_case': result['test_case'],\n",
        "        'question': result['question'],\n",
        "        'domain': result['domain'],\n",
        "        'difficulty': result['difficulty'],\n",
        "        'answer': result['answer'],\n",
        "        'retrieval_metrics': result['retrieval_metrics'],\n",
        "        'answer_metrics': {\n",
        "            k: v for k, v in result['answer_metrics'].items() \n",
        "            if k not in ['key_points_found', 'legal_terms_used']  # Remove lists for JSON\n",
        "        },\n",
        "        'semantic_similarity': result['semantic_similarity'],\n",
        "        'retrieved_docs': result['retrieved_docs']\n",
        "    }\n",
        "    serializable_results.append(serializable_result)\n",
        "\n",
        "# Save to file\n",
        "output_file = \"data/evaluation_results.json\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"üíæ Evaluation results saved to: {output_file}\")\n",
        "\n",
        "# Generate a quick CSV summary for easier analysis\n",
        "summary_data = []\n",
        "for result in evaluation_results:\n",
        "    summary_data.append({\n",
        "        'question': result['question'][:50] + \"...\",\n",
        "        'domain': result['domain'],\n",
        "        'difficulty': result['difficulty'],\n",
        "        'precision_at_5': result['retrieval_metrics']['precision_at_k'],\n",
        "        'key_points_coverage': result['answer_metrics']['key_points_coverage'],\n",
        "        'semantic_similarity': result['semantic_similarity'],\n",
        "        'has_citations': result['answer_metrics']['has_citations'],\n",
        "        'word_count': result['answer_metrics']['word_count']\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_file = \"data/evaluation_summary.csv\"\n",
        "summary_df.to_csv(summary_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"üìä Evaluation summary saved to: {summary_file}\")\n",
        "print(f\"\\nüéØ Use these files to track RAG system improvements over time!\")\n",
        "\n",
        "# Display summary table\n",
        "print(f\"\\nüìã EVALUATION SUMMARY TABLE:\")\n",
        "print(summary_df.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "legalqa",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
