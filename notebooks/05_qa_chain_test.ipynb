{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import faiss\n",
    "import textwrap\n",
    "\n",
    "# Import LangChain and OpenAI components\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import our custom components\n",
    "from src.rag.retriever import CustomRetriever, RerankingRetriever\n",
    "from src.chain.qa_chain import build_qa_chain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set working directory\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "# Load data and models\n",
    "df = pd.read_parquet(os.getenv('PARQUET_PATH'))\n",
    "faiss_index = faiss.read_index(os.getenv('FAISS_INDEX_PATH'))\n",
    "with open(os.getenv('ID_MAPPING_PATH'), 'rb') as f:\n",
    "    id_mapping = pickle.load(f)\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the base retriever\n",
    "custom_retriever = CustomRetriever(\n",
    "    embeddings=embeddings,\n",
    "    faiss_index=faiss_index,\n",
    "    id_mapping=id_mapping,\n",
    "    documents_df=df,\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# 2. Load the reranker prompt\n",
    "with open('src/prompts/reranker_prompt.txt', 'r', encoding='utf-8') as f:\n",
    "    reranker_prompt_template = f.read()\n",
    "reranker_prompt = PromptTemplate.from_template(reranker_prompt_template)\n",
    "\n",
    "# 3. Initialize the reranker LLM\n",
    "reranker_llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# 4. Initialize the RerankingRetriever\n",
    "reranking_retriever = RerankingRetriever(\n",
    "    retriever=custom_retriever,\n",
    "    llm=reranker_llm,\n",
    "    reranker_prompt=reranker_prompt,\n",
    "    embeddings=embeddings,\n",
    "    k=5 # The reranker will return the top 5 documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> [Retriever] Starting document retrieval process...\n",
      "--> [Retriever] Searching FAISS index...\n",
      "--> [Retriever] Found 20 potential chunks from FAISS. Fetching from DB.\n",
      "--> [Retriever] Fetching data from LOCAL PARQUET file...\n",
      "--> [Retriever] Found 0 documents in Parquet file.\n",
      "--> [Retriever] No documents found in DB for the retrieved IDs.\n",
      "❓ Query: Mely volt az utóbbi évek legfontosabb Kúriai döntése?\n",
      "\n",
      "✅ Final Answer:\n",
      "\n",
      "1. Szintetizált Válasz:   A megadott dokumentumok alapján a kérdés nem válaszolható meg.  2.\n",
      "Részletes Elemzés:   * A kérdés megválaszolásához szükséges információ, miszerint mely volt az\n",
      "utóbbi évek legfontosabb Kúriai döntése, nem található meg a rendelkezésre bocsátott\n",
      "dokumentumokban.  3. Következtetés:   A dokumentumok nem tartalmaznak információt az utóbbi évek\n",
      "legfontosabb Kúriai döntéséről.  4. Jogi nyilatkozat:   Ez az elemzés kizárólag a rendelkezésre\n",
      "bocsátott dokumentumrészletek alapján készült, és nem minősül jogi tanácsadásnak.\n"
     ]
    }
   ],
   "source": [
    "# 5. Build the final Question-Answering chain\n",
    "qa_chain = build_qa_chain(retriever=reranking_retriever)\n",
    "\n",
    "# 6. Define the query and invoke the chain\n",
    "query = \"Mely volt az utóbbi évek legfontosabb Kúriai döntése?\"\n",
    "result = qa_chain.invoke(query)\n",
    "\n",
    "# 7. Print the final answer\n",
    "print(f\"❓ Query: {query}\\n\")\n",
    "print(\"✅ Final Answer:\\n\")\n",
    "print(textwrap.fill(result, width=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
